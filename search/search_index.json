{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"\u5173\u4e8e\u672c\u5de5\u7a0b # \u672c\u5de5\u7a0b\u4e13\u6ce8\u4e8e natural language \u76f8\u5173\u7684\u5185\u5bb9\uff0c\u4e00\u822c\u79f0\u4e4b\u4e3a NLP\uff08natural language processing\uff09 \u3002\u5173\u4e8e formal language \u7684\u5185\u5bb9\uff0c\u53c2\u89c1\u5de5\u7a0b automata-and-formal-language \u3002\u4e24\u4e2a\u5de5\u7a0b\u4e2d\u7684\u5185\u5bb9\u662f\u5bc6\u5207\u76f8\u5173\u7684\u3002","title":"Home"},{"location":"#_1","text":"\u672c\u5de5\u7a0b\u4e13\u6ce8\u4e8e natural language \u76f8\u5173\u7684\u5185\u5bb9\uff0c\u4e00\u822c\u79f0\u4e4b\u4e3a NLP\uff08natural language processing\uff09 \u3002\u5173\u4e8e formal language \u7684\u5185\u5bb9\uff0c\u53c2\u89c1\u5de5\u7a0b automata-and-formal-language \u3002\u4e24\u4e2a\u5de5\u7a0b\u4e2d\u7684\u5185\u5bb9\u662f\u5bc6\u5207\u76f8\u5173\u7684\u3002","title":"\u5173\u4e8e\u672c\u5de5\u7a0b"},{"location":"Linguistics/","text":"\u5173\u4e8e\u672c\u7ae0 # NLP\u662f\u4e00\u95e8\u4ea4\u53c9\u5b66\u79d1\uff0c\u51fa\u6765computer science\u5916\uff0c\u5b83\u8fd8\u6d89\u53ca Linguistics \uff08\u8bed\u8a00\u5b66\uff09\u7684\u5f88\u591a\u5185\u5bb9\u3002\u672c\u7ae0\u5c31\u68b3\u7406\u4e00\u4e0b\u5728\u4ece\u4e8bNLP\u65f6\u5019\uff0c\u9700\u8981\u7684\u4e00\u4e9b Linguistics \u7684\u57fa\u7840\u77e5\u8bc6\u3002 Language # \u53c2\u89c1\uff1a \u7ef4\u57fa\u767e\u79d1 Language \u5de5\u7a0b automata-and-formal-language \u7684 Formal-language \u7ae0\u8282 \u6587\u7ae0 Language and program Grammar # \u53c2\u89c1\uff1a \u7ef4\u57fa\u767e\u79d1 Grammar \u5de5\u7a0b automata-and-formal-language \u7684 Formal-language \u7ae0\u8282 \u8bfb\u8005\u9700\u8981\u81f3\u5c11\u5bf9Grammatical theories\u7684\u51e0\u4e2a\u6d41\u6d3e\u6709\u4e00\u4e9b\u8ba4\u77e5\uff0c\u5c24\u5176\u662f Generative grammar \u6d41\u6d3e\u3002 Phrase structure grammar VS Dependency grammar # \u5728NLP\u9886\u57df\uff0c\u8fd9\u4e24\u79cdgrammar\u662f\u6700\u4e3a\u5e38\u89c1\u7684\uff0c\u4e24\u8005\u672c\u8d28\u7684\u5dee\u5f02\u5728\u4e8e\u63cf\u8ff0\u8bed\u8a00\u7684\u7ed3\u6784\u65f6\u91c7\u7528\u7684relation\u4e0d\u540c\uff0c Phrase structure grammar \u6240\u91c7\u7528\u7684\u662f Constituency relation \uff0c\u800c Dependency grammar \u6240\u91c7\u7528\u7684\u662f Dependency relation \u3002","title":"Introduction"},{"location":"Linguistics/#_1","text":"NLP\u662f\u4e00\u95e8\u4ea4\u53c9\u5b66\u79d1\uff0c\u51fa\u6765computer science\u5916\uff0c\u5b83\u8fd8\u6d89\u53ca Linguistics \uff08\u8bed\u8a00\u5b66\uff09\u7684\u5f88\u591a\u5185\u5bb9\u3002\u672c\u7ae0\u5c31\u68b3\u7406\u4e00\u4e0b\u5728\u4ece\u4e8bNLP\u65f6\u5019\uff0c\u9700\u8981\u7684\u4e00\u4e9b Linguistics \u7684\u57fa\u7840\u77e5\u8bc6\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Linguistics/#language","text":"\u53c2\u89c1\uff1a \u7ef4\u57fa\u767e\u79d1 Language \u5de5\u7a0b automata-and-formal-language \u7684 Formal-language \u7ae0\u8282 \u6587\u7ae0 Language and program","title":"Language"},{"location":"Linguistics/#grammar","text":"\u53c2\u89c1\uff1a \u7ef4\u57fa\u767e\u79d1 Grammar \u5de5\u7a0b automata-and-formal-language \u7684 Formal-language \u7ae0\u8282 \u8bfb\u8005\u9700\u8981\u81f3\u5c11\u5bf9Grammatical theories\u7684\u51e0\u4e2a\u6d41\u6d3e\u6709\u4e00\u4e9b\u8ba4\u77e5\uff0c\u5c24\u5176\u662f Generative grammar \u6d41\u6d3e\u3002","title":"Grammar"},{"location":"Linguistics/#phrase-structure-grammar-vs-dependency-grammar","text":"\u5728NLP\u9886\u57df\uff0c\u8fd9\u4e24\u79cdgrammar\u662f\u6700\u4e3a\u5e38\u89c1\u7684\uff0c\u4e24\u8005\u672c\u8d28\u7684\u5dee\u5f02\u5728\u4e8e\u63cf\u8ff0\u8bed\u8a00\u7684\u7ed3\u6784\u65f6\u91c7\u7528\u7684relation\u4e0d\u540c\uff0c Phrase structure grammar \u6240\u91c7\u7528\u7684\u662f Constituency relation \uff0c\u800c Dependency grammar \u6240\u91c7\u7528\u7684\u662f Dependency relation \u3002","title":"Phrase structure grammar VS Dependency grammar"},{"location":"Linguistics/Part-of-speech/","text":"Part of speech # \u672c\u6587\u7684\u5185\u5bb9\u57fa\u4e8e\uff1a \u7ef4\u57fa\u767e\u79d1 Part of speech Natural Language Processing with Python \u7684 5. Categorizing and Tagging Words What is \u201cpart of speech\u201d\uff1f # \"part of speech\"\u5373\u8bcd\u6027\uff0c\u5982\u679c\u4ee5machine learning\u7684\u89d2\u5ea6\u6765\u770b\u7684\u8bdd\uff0c\u201cpart of speech\u201d\u975e\u5e38\u7c7b\u4f3c\u4e8e\u201clabel\u201d\uff0c\u5373\u5b83\u8868\u793a\u7684\u662f\u8bcd\u8bed\u7684 \u7c7b\u522b \uff08\u540e\u6587\u4e2d\u4f1a\u51fa\u73b0\u201dword class\u201c\u8fd9\u4e2a\u8bcd\uff0c\u663e\u7136\u8fd9\u4e2a\u8bcd\u662f\u66f4\u52a0\u80fd\u591f\u4f53\u73b0\u5b83\u7684\u542b\u4e49\u7684\uff09\uff0c\u8fd9\u4e00\u70b9\u5728 Natural Language Processing with Python \u7684 5. Categorizing and Tagging Words \u7684\u540d\u79f0\u4e2d\u7684\u201cCategorizing\"\uff08\u5206\u7c7b\uff09\u4e2d\u4f53\u73b0\u51fa\u6765\u4e86\u3002\u663e\u7136\uff0c\u65e2\u7136\u80fd\u591f\u8fdb\u884c\u5206\u7c7b\uff0c\u90a3\u4e48\u5404\u79cdpart of speech\uff08word class\uff09\u80af\u5b9a\u6709\u7740\u663e\u8457\u7684\u7279\u5f81\u3002 \u6709\u4e86\u8fd9\u4e9b\u8ba4\u77e5\uff0c\u5c31\u80fd\u591f\u7406\u89e3\u7ef4\u57fa\u767e\u79d1\u7684 Part of speech \u4e2d\u7684\u5b9a\u4e49\u4e86\uff1a In traditional grammar , a part of speech (abbreviated form: PoS or POS ) is a category of words (or, more generally, of lexical items ) that have similar grammatical properties. Words that are assigned to the same part of speech generally display similar syntactic behavior\u2014they play similar roles within the grammatical structure of sentences\u2014and sometimes similar morphology in that they undergo inflection for similar properties. \u4e0b\u9762\u7ed9\u51fa\u4e86part of speech\u7684\u4e00\u4e2a\u4f8b\u5b50\uff1a Commonly listed English parts of speech are noun , verb , adjective , adverb , pronoun , preposition , conjunction , interjection , and sometimes numeral , article , or determiner . Other terms than part of speech \u2014particularly in modern linguistic classifications, which often make more precise distinctions than the traditional scheme does\u2014include word class , lexical class , and lexical category . \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u4e0d\u540c\u7684\u8bed\u8a00\u6709\u7740\u4e0d\u540c\u7684part of speech Because of such variation in the number of categories and their identifying properties, analysis of parts of speech must be done for each individual language. \u5982\u4f55\u5b9a\u4e49\uff08\u521b\u9020\uff09part of speech\uff1f # \u4e00\u95e8\u8bed\u8a00\u7684\u53ef\u7528\u4f7f\u7528\u7684Part of speech\u4e0d\u662f\u56fa\u5b9a\u7684\uff0c\u7814\u7a76\u4eba\u5458\u662f\u53ef\u4ee5\u6839\u636e\u9700\u6c42\u6765\u521b\u9020\u9002\u5408\u4e8e\u7279\u5b9a\u95ee\u9898\u7684part of speech\uff08\u5176\u5b9e\u5c31\u662f\u5b9a\u4e49\u7c7b\u522b\uff0c\u4f46\u662f\u663e\u7136\u5728\u8fdb\u884c\u5b9a\u4e49\u7684\u65f6\u5019\uff0c\u5e94\u8be5\u662f\u9700\u8981\u8003\u8651\u8bed\u8a00\u5b66\u7684\u7406\u8bba\u7684\uff09\uff0c\u8fd9\u4e9bpart of speech\u79f0\u4e3a\u201c Tag sets \u201d\u3002\u90a3\u5982\u4f55\u6765\u8fdb\u884c\u521b\u9020\u5462\uff1f\u4e0b\u9762\u7ed9\u51fa\u4e86\u4e00\u4e9b\u6709\u53c2\u8003\u4ef7\u503c\u7684\u5185\u5bb9\uff1a Functional classification 7 How to Determine the Category of a Word \u5982\u4f55\u8ba9\u8ba1\u7b97\u673a\u6765\u81ea\u52a8\u5730\u8fdb\u884c\u201c\u5206\u7c7b\u201d\uff1f # \u6211\u4eec\u5df2\u7ecf\u4e86\u89e3\u4e86part of speech\u7684\u542b\u4e49\uff0c\u77e5\u9053\u5b83\u672c\u8d28\u4e0a\u5c31\u662f\u7c7b\u522b\u3002\u7ed9\u5b9a\u4e00\u6bb5\u8bdd\uff0c\u6211\u4eec\u4eba\u7c7b\u662f\u53ef\u4ee5\u975e\u5e38\u8f7b\u677e\u5730\u6307\u51fa\u54ea\u4e9b\u8bcd\u662f\u5c5e\u4e8e\u54ea\u79cdpart of speech\uff0c\u90a3\u5982\u4f55\u8ba9computer\u4e5f\u83b7\u5f97\u8fd9\u79cd\u80fd\u529b\u5462\uff1f\u8fd9\u5c31\u662f\u672c\u8282\u6807\u9898\u7684\u6240\u63d0\u51fa\u7684\u95ee\u9898,\uff0c\u8fd9\u4e2a\u95ee\u9898\u662fNLP\u9886\u57df\u7684\u7ecf\u5178\u95ee\u9898\uff0c\u53eb\u505a Part-of-speech tagging \uff0c\u5728\u4e0b\u4e00\u7ae0\u8282\u7684 Part-of-speech-tagging \u4e2d\u5bf9\u6b64\u8fdb\u884c\u4e13\u95e8\u8ba8\u8bba\u3002","title":"Part-of-speech"},{"location":"Linguistics/Part-of-speech/#part-of-speech","text":"\u672c\u6587\u7684\u5185\u5bb9\u57fa\u4e8e\uff1a \u7ef4\u57fa\u767e\u79d1 Part of speech Natural Language Processing with Python \u7684 5. Categorizing and Tagging Words","title":"Part of speech"},{"location":"Linguistics/Part-of-speech/#what-is-part-of-speech","text":"\"part of speech\"\u5373\u8bcd\u6027\uff0c\u5982\u679c\u4ee5machine learning\u7684\u89d2\u5ea6\u6765\u770b\u7684\u8bdd\uff0c\u201cpart of speech\u201d\u975e\u5e38\u7c7b\u4f3c\u4e8e\u201clabel\u201d\uff0c\u5373\u5b83\u8868\u793a\u7684\u662f\u8bcd\u8bed\u7684 \u7c7b\u522b \uff08\u540e\u6587\u4e2d\u4f1a\u51fa\u73b0\u201dword class\u201c\u8fd9\u4e2a\u8bcd\uff0c\u663e\u7136\u8fd9\u4e2a\u8bcd\u662f\u66f4\u52a0\u80fd\u591f\u4f53\u73b0\u5b83\u7684\u542b\u4e49\u7684\uff09\uff0c\u8fd9\u4e00\u70b9\u5728 Natural Language Processing with Python \u7684 5. Categorizing and Tagging Words \u7684\u540d\u79f0\u4e2d\u7684\u201cCategorizing\"\uff08\u5206\u7c7b\uff09\u4e2d\u4f53\u73b0\u51fa\u6765\u4e86\u3002\u663e\u7136\uff0c\u65e2\u7136\u80fd\u591f\u8fdb\u884c\u5206\u7c7b\uff0c\u90a3\u4e48\u5404\u79cdpart of speech\uff08word class\uff09\u80af\u5b9a\u6709\u7740\u663e\u8457\u7684\u7279\u5f81\u3002 \u6709\u4e86\u8fd9\u4e9b\u8ba4\u77e5\uff0c\u5c31\u80fd\u591f\u7406\u89e3\u7ef4\u57fa\u767e\u79d1\u7684 Part of speech \u4e2d\u7684\u5b9a\u4e49\u4e86\uff1a In traditional grammar , a part of speech (abbreviated form: PoS or POS ) is a category of words (or, more generally, of lexical items ) that have similar grammatical properties. Words that are assigned to the same part of speech generally display similar syntactic behavior\u2014they play similar roles within the grammatical structure of sentences\u2014and sometimes similar morphology in that they undergo inflection for similar properties. \u4e0b\u9762\u7ed9\u51fa\u4e86part of speech\u7684\u4e00\u4e2a\u4f8b\u5b50\uff1a Commonly listed English parts of speech are noun , verb , adjective , adverb , pronoun , preposition , conjunction , interjection , and sometimes numeral , article , or determiner . Other terms than part of speech \u2014particularly in modern linguistic classifications, which often make more precise distinctions than the traditional scheme does\u2014include word class , lexical class , and lexical category . \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u4e0d\u540c\u7684\u8bed\u8a00\u6709\u7740\u4e0d\u540c\u7684part of speech Because of such variation in the number of categories and their identifying properties, analysis of parts of speech must be done for each individual language.","title":"What is \u201cpart of speech\u201d\uff1f"},{"location":"Linguistics/Part-of-speech/#part-of-speech_1","text":"\u4e00\u95e8\u8bed\u8a00\u7684\u53ef\u7528\u4f7f\u7528\u7684Part of speech\u4e0d\u662f\u56fa\u5b9a\u7684\uff0c\u7814\u7a76\u4eba\u5458\u662f\u53ef\u4ee5\u6839\u636e\u9700\u6c42\u6765\u521b\u9020\u9002\u5408\u4e8e\u7279\u5b9a\u95ee\u9898\u7684part of speech\uff08\u5176\u5b9e\u5c31\u662f\u5b9a\u4e49\u7c7b\u522b\uff0c\u4f46\u662f\u663e\u7136\u5728\u8fdb\u884c\u5b9a\u4e49\u7684\u65f6\u5019\uff0c\u5e94\u8be5\u662f\u9700\u8981\u8003\u8651\u8bed\u8a00\u5b66\u7684\u7406\u8bba\u7684\uff09\uff0c\u8fd9\u4e9bpart of speech\u79f0\u4e3a\u201c Tag sets \u201d\u3002\u90a3\u5982\u4f55\u6765\u8fdb\u884c\u521b\u9020\u5462\uff1f\u4e0b\u9762\u7ed9\u51fa\u4e86\u4e00\u4e9b\u6709\u53c2\u8003\u4ef7\u503c\u7684\u5185\u5bb9\uff1a Functional classification 7 How to Determine the Category of a Word","title":"\u5982\u4f55\u5b9a\u4e49\uff08\u521b\u9020\uff09part of speech\uff1f"},{"location":"Linguistics/Part-of-speech/#_1","text":"\u6211\u4eec\u5df2\u7ecf\u4e86\u89e3\u4e86part of speech\u7684\u542b\u4e49\uff0c\u77e5\u9053\u5b83\u672c\u8d28\u4e0a\u5c31\u662f\u7c7b\u522b\u3002\u7ed9\u5b9a\u4e00\u6bb5\u8bdd\uff0c\u6211\u4eec\u4eba\u7c7b\u662f\u53ef\u4ee5\u975e\u5e38\u8f7b\u677e\u5730\u6307\u51fa\u54ea\u4e9b\u8bcd\u662f\u5c5e\u4e8e\u54ea\u79cdpart of speech\uff0c\u90a3\u5982\u4f55\u8ba9computer\u4e5f\u83b7\u5f97\u8fd9\u79cd\u80fd\u529b\u5462\uff1f\u8fd9\u5c31\u662f\u672c\u8282\u6807\u9898\u7684\u6240\u63d0\u51fa\u7684\u95ee\u9898,\uff0c\u8fd9\u4e2a\u95ee\u9898\u662fNLP\u9886\u57df\u7684\u7ecf\u5178\u95ee\u9898\uff0c\u53eb\u505a Part-of-speech tagging \uff0c\u5728\u4e0b\u4e00\u7ae0\u8282\u7684 Part-of-speech-tagging \u4e2d\u5bf9\u6b64\u8fdb\u884c\u4e13\u95e8\u8ba8\u8bba\u3002","title":"\u5982\u4f55\u8ba9\u8ba1\u7b97\u673a\u6765\u81ea\u52a8\u5730\u8fdb\u884c\u201c\u5206\u7c7b\u201d\uff1f"},{"location":"NLP/NLP/","text":"NLP # \u672c\u4e66\u7684\u7b2c\u4e00\u7ae0\u4e2d\u5c06\u4eba\u7c7b\u6240\u9762\u4e34\u7684\u95ee\u9898\u5206\u4e3a\u4e86\u4e24\u7c7b\uff1a problems that can be described by a list of formal, mathematical rules problems that can not be described by a list of formal, mathematical rules Language\u5176\u5b9e\u6709\u5bf9\u5e94\u7684\u4e24\u7c7b\uff1a formal language\uff0c\u6bd4\u5982programming language natural language \u663e\u7136\uff0c\u5bf9\u4e8eformal language\uff0c\u4f20\u7edf\u7684\u8ba1\u7b97\u673a\u7a0b\u5e8f\u5c31\u80fd\u591f\u641e\u5b9a\uff0c\u4f46\u662f\u5bf9\u4e8enatural language\uff0c\u4f20\u7edf\u7684\u8ba1\u7b97\u673a\u7a0b\u5e8f\u662f\u65e0\u6cd5\u89e3\u51b3\u7684\uff0c\u5f53\u4eca\u7684\u89e3\u51b3\u65b9\u6848\u5c31\u662f\u672c\u4e66\u6240\u63cf\u8ff0\u7684machine learning\uff0c\u672c\u7ae0\u6240\u8981\u63a2\u7d22\u7684\u6b63\u662f\u8fd9\u4e2a\u4e3b\u9898\uff0c\u5373\u5982\u4f55\u4f7f\u7528machine learning\u6765\u5b9e\u73b0NLP\u3002 \u5173\u4e8eNLP\uff0c\u53c2\u89c1\uff1a \u7ef4\u57fa\u767e\u79d1 Natural language processing","title":"NLP"},{"location":"NLP/NLP/#nlp","text":"\u672c\u4e66\u7684\u7b2c\u4e00\u7ae0\u4e2d\u5c06\u4eba\u7c7b\u6240\u9762\u4e34\u7684\u95ee\u9898\u5206\u4e3a\u4e86\u4e24\u7c7b\uff1a problems that can be described by a list of formal, mathematical rules problems that can not be described by a list of formal, mathematical rules Language\u5176\u5b9e\u6709\u5bf9\u5e94\u7684\u4e24\u7c7b\uff1a formal language\uff0c\u6bd4\u5982programming language natural language \u663e\u7136\uff0c\u5bf9\u4e8eformal language\uff0c\u4f20\u7edf\u7684\u8ba1\u7b97\u673a\u7a0b\u5e8f\u5c31\u80fd\u591f\u641e\u5b9a\uff0c\u4f46\u662f\u5bf9\u4e8enatural language\uff0c\u4f20\u7edf\u7684\u8ba1\u7b97\u673a\u7a0b\u5e8f\u662f\u65e0\u6cd5\u89e3\u51b3\u7684\uff0c\u5f53\u4eca\u7684\u89e3\u51b3\u65b9\u6848\u5c31\u662f\u672c\u4e66\u6240\u63cf\u8ff0\u7684machine learning\uff0c\u672c\u7ae0\u6240\u8981\u63a2\u7d22\u7684\u6b63\u662f\u8fd9\u4e2a\u4e3b\u9898\uff0c\u5373\u5982\u4f55\u4f7f\u7528machine learning\u6765\u5b9e\u73b0NLP\u3002 \u5173\u4e8eNLP\uff0c\u53c2\u89c1\uff1a \u7ef4\u57fa\u767e\u79d1 Natural language processing","title":"NLP"},{"location":"NLP/Task-of-NLP/","text":"NLP task # \u672c\u8282\u5bf9 NLP-progress \u4e2d\u6240\u603b\u7ed3\u7684NLP task\u8fdb\u884c\u603b\u7ed3\uff0c\u5206\u6790\u3002 \u548c\u8bed\u6cd5\u7ed3\u6784\u76f8\u5173\u7684NLP task # \u8fd9\u7c7bNLP task\u4ec5\u4ec5\u5bf9\u53e5\u5b50\u7684\u8bed\u6cd5\u7ed3\u6784\u8fdb\u884c\u5206\u6790\uff0c\u4e0d\u6d89\u53ca\u8bed\u4e49\u3002\u5728 \u8bed\u8a00\u5b66 \u4e2d\uff0c\u4f7f\u7528 Grammar \u6765\u63cf\u8ff0\u8bed\u8a00\uff0c\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u4e2d\uff0c parsing \u5f80\u5f80\u6307\u8fdb\u884c\u8bed\u6cd5\u5206\u6790\uff0c\u6240\u4ee5\u4e0b\u9762\u540d\u79f0\u4e2d\u5e26\u6709parsing\u7684\u90fd\u5177\u6709\u6b64\u542b\u4e49\u3002 CCG # Combinatory categorial grammar \uff0c\u9700\u8981\u6ce8\u610f\u7684\u662f Combinatory categorial grammar \u662f\u4e00\u79cd phrase structure grammar \u3002 Constituency parsing # phrase structure grammar Dependency parsing # \u4e0d\u786e\u5b9a\u662f\u5426\u662f Dependency grammar Shallow syntax # \u5b83\u8fd8\u6709\u5176\u5b83\u7684\u540d\u79f0\uff1aChunking\u3001shallow parsing\uff0c\u4e0e\u524d\u9762\u51e0\u79cdparsing\u76f8\u6bd4\uff0c\u5b83\u662f\u6bd4\u8f83\u201c\u6d45\u5c42\u201d\u7684parsing\u3002 \u8bed\u4e49\u76f8\u5173\u7684 # Coreference resolution # Semantic textual similarity # Semantic parsing # \u548c\u8bed\u97f3\u76f8\u5173\u7684NLP task # Automatic speech recognition \u5176\u4ed6 # Language modeling Common sense Natural language inference","title":"Task-of-NLP"},{"location":"NLP/Task-of-NLP/#nlp-task","text":"\u672c\u8282\u5bf9 NLP-progress \u4e2d\u6240\u603b\u7ed3\u7684NLP task\u8fdb\u884c\u603b\u7ed3\uff0c\u5206\u6790\u3002","title":"NLP task"},{"location":"NLP/Task-of-NLP/#nlp-task_1","text":"\u8fd9\u7c7bNLP task\u4ec5\u4ec5\u5bf9\u53e5\u5b50\u7684\u8bed\u6cd5\u7ed3\u6784\u8fdb\u884c\u5206\u6790\uff0c\u4e0d\u6d89\u53ca\u8bed\u4e49\u3002\u5728 \u8bed\u8a00\u5b66 \u4e2d\uff0c\u4f7f\u7528 Grammar \u6765\u63cf\u8ff0\u8bed\u8a00\uff0c\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u4e2d\uff0c parsing \u5f80\u5f80\u6307\u8fdb\u884c\u8bed\u6cd5\u5206\u6790\uff0c\u6240\u4ee5\u4e0b\u9762\u540d\u79f0\u4e2d\u5e26\u6709parsing\u7684\u90fd\u5177\u6709\u6b64\u542b\u4e49\u3002","title":"\u548c\u8bed\u6cd5\u7ed3\u6784\u76f8\u5173\u7684NLP task"},{"location":"NLP/Task-of-NLP/#ccg","text":"Combinatory categorial grammar \uff0c\u9700\u8981\u6ce8\u610f\u7684\u662f Combinatory categorial grammar \u662f\u4e00\u79cd phrase structure grammar \u3002","title":"CCG"},{"location":"NLP/Task-of-NLP/#constituency-parsing","text":"phrase structure grammar","title":"Constituency parsing"},{"location":"NLP/Task-of-NLP/#dependency-parsing","text":"\u4e0d\u786e\u5b9a\u662f\u5426\u662f Dependency grammar","title":"Dependency parsing"},{"location":"NLP/Task-of-NLP/#shallow-syntax","text":"\u5b83\u8fd8\u6709\u5176\u5b83\u7684\u540d\u79f0\uff1aChunking\u3001shallow parsing\uff0c\u4e0e\u524d\u9762\u51e0\u79cdparsing\u76f8\u6bd4\uff0c\u5b83\u662f\u6bd4\u8f83\u201c\u6d45\u5c42\u201d\u7684parsing\u3002","title":"Shallow syntax"},{"location":"NLP/Task-of-NLP/#_1","text":"","title":"\u8bed\u4e49\u76f8\u5173\u7684"},{"location":"NLP/Task-of-NLP/#coreference-resolution","text":"","title":"Coreference resolution"},{"location":"NLP/Task-of-NLP/#semantic-textual-similarity","text":"","title":"Semantic textual similarity"},{"location":"NLP/Task-of-NLP/#semantic-parsing","text":"","title":"Semantic parsing"},{"location":"NLP/Task-of-NLP/#nlp-task_2","text":"Automatic speech recognition","title":"\u548c\u8bed\u97f3\u76f8\u5173\u7684NLP task"},{"location":"NLP/Task-of-NLP/#_2","text":"Language modeling Common sense Natural language inference","title":"\u5176\u4ed6"},{"location":"NLP/Book-Natural-Language-Processing-with-Python/","text":"\u5173\u4e8e\u672c\u7ae0 # \u672c\u7ae0\u8bb0\u5f55\u5728\u9605\u8bfb Natural Language Processing with Python \u7684\u7b14\u8bb0\u3002 Natural Language Processing with Python \u662f\u4e00\u672c\u5f00\u6e90\u4e66\u7c4d\uff0c\u539f\u4e66\u5185\u5bb9\u6bd4\u8f83\u901a\u4fd7\u6613\u61c2\uff0c\u53ef\u4ee5\u4f5c\u4e3a\u5165\u95e8NLP\u7684\u8bfb\u7269\u3002 \u5728\u9605\u8bfb\u7684\u65f6\u5019\uff0c\u6211\u4eec\u5e94\u8be5\u9009\u62e9\u6027\u5730\u53bb\u9605\u8bfb\uff0c\u5bf9\u4e8e\u719f\u6089python\u7684\u4eba\uff0c\u53ef\u4ee5\u76f4\u63a5pass\u6389\u539f\u4e66\u4e2d\u5173\u4e8epython\u7684\u5185\u5bb9\u3002\u539f\u4e66\u4ecechapter 5\u5f00\u59cb\u8bb2\u8ff0NLP\u76f8\u5173\u5185\u5bb9\u3002 \u6211\u89c9\u5f97\u8fd9\u672c\u4e66\u7684\u6700\u5927\u4ef7\u503c\u5728\u4e8e\uff1a \u7ed3\u5408\u5177\u4f53\u4f8b\u5b50\u8bb2\u89e3\u4e86\u8bed\u8a00\u5b66\u4e2d\u7684\u7684\u4e00\u4e9b\u57fa\u7840\u6982\u5ff5 \u7ed9\u51fa\u4e86\u89e3\u51b3\u4e00\u4e9bNLP task\u7684Pipeline Architecture\uff0c\u8bfb\u8005\u53ef\u4ee5\u53c2\u8003\u8fdb\u884c\u5b9e\u8df5\uff0c\u4e0b\u9762\u5bf9\u6b64\u8fdb\u884c\u4e86\u603b\u7ed3\u3002 \u8bfb\u8005\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u867d\u7136\u4e66\u4e2d\u7ed9\u51fa\u4e86\u4e00\u4e9bNLP task\u7684\u5b9e\u73b0\u65b9\u5f0f\uff0c\u4f46\u662f\u968f\u7740\u6280\u672f\u7684\u8fdb\u6b65\uff0c\u4e0d\u65ad\u6d8c\u73b0\u51fa\u4e86\u89e3\u51b3\u8fd9\u4e9b\u5df2\u77e5\u7684NLP task\u7684\u65b0\u6280\u672f\uff0c\u6240\u4ee5\u4e0d\u80fd\u591f\u62d8\u6ce5\u4e8e\u4e66\u4e2d\u7684\u89e3\u51b3\u65b9\u5f0f\uff0c\u800c\u5e94\u8be5\u5b66\u4e60\u66f4\u52a0\u5148\u8fdb\u7684\u89e3\u51b3\u65b9\u5f0f\u3002 \u5185\u5bb9\u6982\u8981 # \u53ef\u4ee5\u76f4\u63a5\u4ecechapter 5 Categorizing and Tagging Words \u5f00\u59cb\u9605\u8bfb\uff1bchapter 6 Learning to Classify Text \u8bb2\u6587\u672c\u5206\u7c7b\uff1bchapter 7 Extracting Information from Text \u8bb2\u5982\u4f55\u4ece\u6587\u672c\u4e2d\u62bd\u53d6\u4fe1\u606f\uff0c\u5176\u4e2d\u7ed9\u51fa\u7684 Information Extraction Architecture \u662f\u6bd4\u8f83\u5177\u6709\u542f\u53d1\u610f\u4e49\u7684\uff1bchapter 8 Analyzing Sentence Structure \u5176\u5b9e\u8bb2\u8ff0\u7684\u662fformal language\u4e2dparsing\u7684\u65b9\u6cd5\u3002 Pipeline Architecture # Simple Pipeline Architecture for a Spoken Dialogue System # \u5728 chapter 1. Language Processing and Python \u76845.5 Spoken Dialog Systems\u4e2d\u7ed9\u51fa\u4e86\u201c Simple Pipeline Architecture for a Spoken Dialogue System \u201d Figure 5.1 : Simple Pipeline Architecture for a Spoken Dialogue System: Spoken input (top left) is analyzed, words are recognized, sentences are parsed and interpreted in context, application-specific actions take place (top right); a response is planned, realized as a syntactic structure, then to suitably inflected words, and finally to spoken output; different types of linguistic knowledge inform each stage of the process.","title":"Introduction"},{"location":"NLP/Book-Natural-Language-Processing-with-Python/#_1","text":"\u672c\u7ae0\u8bb0\u5f55\u5728\u9605\u8bfb Natural Language Processing with Python \u7684\u7b14\u8bb0\u3002 Natural Language Processing with Python \u662f\u4e00\u672c\u5f00\u6e90\u4e66\u7c4d\uff0c\u539f\u4e66\u5185\u5bb9\u6bd4\u8f83\u901a\u4fd7\u6613\u61c2\uff0c\u53ef\u4ee5\u4f5c\u4e3a\u5165\u95e8NLP\u7684\u8bfb\u7269\u3002 \u5728\u9605\u8bfb\u7684\u65f6\u5019\uff0c\u6211\u4eec\u5e94\u8be5\u9009\u62e9\u6027\u5730\u53bb\u9605\u8bfb\uff0c\u5bf9\u4e8e\u719f\u6089python\u7684\u4eba\uff0c\u53ef\u4ee5\u76f4\u63a5pass\u6389\u539f\u4e66\u4e2d\u5173\u4e8epython\u7684\u5185\u5bb9\u3002\u539f\u4e66\u4ecechapter 5\u5f00\u59cb\u8bb2\u8ff0NLP\u76f8\u5173\u5185\u5bb9\u3002 \u6211\u89c9\u5f97\u8fd9\u672c\u4e66\u7684\u6700\u5927\u4ef7\u503c\u5728\u4e8e\uff1a \u7ed3\u5408\u5177\u4f53\u4f8b\u5b50\u8bb2\u89e3\u4e86\u8bed\u8a00\u5b66\u4e2d\u7684\u7684\u4e00\u4e9b\u57fa\u7840\u6982\u5ff5 \u7ed9\u51fa\u4e86\u89e3\u51b3\u4e00\u4e9bNLP task\u7684Pipeline Architecture\uff0c\u8bfb\u8005\u53ef\u4ee5\u53c2\u8003\u8fdb\u884c\u5b9e\u8df5\uff0c\u4e0b\u9762\u5bf9\u6b64\u8fdb\u884c\u4e86\u603b\u7ed3\u3002 \u8bfb\u8005\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u867d\u7136\u4e66\u4e2d\u7ed9\u51fa\u4e86\u4e00\u4e9bNLP task\u7684\u5b9e\u73b0\u65b9\u5f0f\uff0c\u4f46\u662f\u968f\u7740\u6280\u672f\u7684\u8fdb\u6b65\uff0c\u4e0d\u65ad\u6d8c\u73b0\u51fa\u4e86\u89e3\u51b3\u8fd9\u4e9b\u5df2\u77e5\u7684NLP task\u7684\u65b0\u6280\u672f\uff0c\u6240\u4ee5\u4e0d\u80fd\u591f\u62d8\u6ce5\u4e8e\u4e66\u4e2d\u7684\u89e3\u51b3\u65b9\u5f0f\uff0c\u800c\u5e94\u8be5\u5b66\u4e60\u66f4\u52a0\u5148\u8fdb\u7684\u89e3\u51b3\u65b9\u5f0f\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"NLP/Book-Natural-Language-Processing-with-Python/#_2","text":"\u53ef\u4ee5\u76f4\u63a5\u4ecechapter 5 Categorizing and Tagging Words \u5f00\u59cb\u9605\u8bfb\uff1bchapter 6 Learning to Classify Text \u8bb2\u6587\u672c\u5206\u7c7b\uff1bchapter 7 Extracting Information from Text \u8bb2\u5982\u4f55\u4ece\u6587\u672c\u4e2d\u62bd\u53d6\u4fe1\u606f\uff0c\u5176\u4e2d\u7ed9\u51fa\u7684 Information Extraction Architecture \u662f\u6bd4\u8f83\u5177\u6709\u542f\u53d1\u610f\u4e49\u7684\uff1bchapter 8 Analyzing Sentence Structure \u5176\u5b9e\u8bb2\u8ff0\u7684\u662fformal language\u4e2dparsing\u7684\u65b9\u6cd5\u3002","title":"\u5185\u5bb9\u6982\u8981"},{"location":"NLP/Book-Natural-Language-Processing-with-Python/#pipeline-architecture","text":"","title":"Pipeline Architecture"},{"location":"NLP/Book-Natural-Language-Processing-with-Python/#simple-pipeline-architecture-for-a-spoken-dialogue-system","text":"\u5728 chapter 1. Language Processing and Python \u76845.5 Spoken Dialog Systems\u4e2d\u7ed9\u51fa\u4e86\u201c Simple Pipeline Architecture for a Spoken Dialogue System \u201d Figure 5.1 : Simple Pipeline Architecture for a Spoken Dialogue System: Spoken input (top left) is analyzed, words are recognized, sentences are parsed and interpreted in context, application-specific actions take place (top right); a response is planned, realized as a syntactic structure, then to suitably inflected words, and finally to spoken output; different types of linguistic knowledge inform each stage of the process.","title":"Simple Pipeline Architecture for a Spoken Dialogue System"},{"location":"NLP/Part-of-speech-tagging/","text":"\u5173\u4e8e\u672c\u7ae0 # \u672c\u7ae0\u63cf\u8ff0NLP\u4efb\u52a1Part-of-speech tagging\uff0c\u53c2\u8003\u5185\u5bb9\u5982\u4e0b\uff1a \u7ef4\u57fa\u767e\u79d1 Part-of-speech tagging NLP-progress \u7684 Part-of-speech tagging Natural Language Processing with Python \u5173\u4e8epart of speech\uff0c\u53c2\u89c1Linguistics\u7ae0\u8282\u3002","title":"Introduction"},{"location":"NLP/Part-of-speech-tagging/#_1","text":"\u672c\u7ae0\u63cf\u8ff0NLP\u4efb\u52a1Part-of-speech tagging\uff0c\u53c2\u8003\u5185\u5bb9\u5982\u4e0b\uff1a \u7ef4\u57fa\u767e\u79d1 Part-of-speech tagging NLP-progress \u7684 Part-of-speech tagging Natural Language Processing with Python \u5173\u4e8epart of speech\uff0c\u53c2\u89c1Linguistics\u7ae0\u8282\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"NLP/shallow-parsing/Shallow-parsing/","text":"Shallow parsing Shallow parsing #","title":"Shallow-parsing"},{"location":"NLP/shallow-parsing/Shallow-parsing/#shallow-parsing","text":"","title":"Shallow parsing"},{"location":"NLP/shallow-parsing/paper-Shallow-Parsing-with-Conditional-Random-Fields/","text":"Shallow Parsing with Conditional Random Fields Shallow Parsing with Conditional Random Fields #","title":"paper Shallow Parsing with Conditional Random Fields"},{"location":"NLP/shallow-parsing/paper-Shallow-Parsing-with-Conditional-Random-Fields/#shallow-parsing-with-conditional-random-fields","text":"","title":"Shallow Parsing with Conditional Random Fields"},{"location":"NLTK/","text":"\u5173\u4e8e\u672c\u7ae0 # Natural Language Toolkit","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"NLTK/#_1","text":"Natural Language Toolkit","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"NLTK/docs/","text":"\u8fd9\u662f\u6211\u9605\u8bfb Natural Language Processing with Python \u7684\u7b14\u8bb0\u3002 \u6211\u662f\u5728\u6709\u4e00\u5b9a\u7684NLP\u77e5\u8bc6\u7684\u57fa\u7840\u540e\u6765\u9605\u8bfb\u8fd9\u672c\u4e66\u7684\uff0c\u8bfb\u540e\u7684\u6536\u83b7\u6709\uff1a \u8fd9\u4e2a\u4e66\u4e3a\u8bfb\u8005\u642d\u5efa\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684NLP\u7cfb\u7edf\u7684\u6784\u6210\u90e8\u4ef6\uff0c\u5e76\u5206\u7ae0\u8282\u8fdb\u884c\u63cf\u8ff0\u3002\u6211\u89c9\u5f97\u4f5c\u8005\u7684\u8fd9\u79cd\u5b89\u6392\u8fd9\u662f\u975e\u5e38\u597d\u7684\uff0c\u8bfb\u5b8c\u540e\uff0c\u8bfb\u8005\u80fd\u591f\u6e05\u695a\u5730\u77e5\u9053\u9700\u8981\u505a\u54ea\u4e9b\u4e8b\u60c5\uff1b \u81f3\u4e8e\u6bcf\u4e00\u90e8\u4ef6\u7684\u5b9e\u73b0\u65b9\u5f0f\uff0c\u4e66\u4e2d\u5e76\u6ca1\u6709\u7ed9\u51fa\u5177\u4f53\u7684\u5b9e\u73b0\u65b9\u5f0f\uff0c\u8fd9\u662f\u53ef\u60f3\u800c\u77e5\u7684\u3002\u56e0\u4e3a\u968f\u7740\u6280\u672f\u7684\u6f14\u8fdb\uff0c\u4e0d\u65ad\u5730\u6709\u5168\u65b0\u7684\u6280\u672f\u51fa\u73b0\u3002\u4f46\u662f\u65e0\u8bba\u6280\u672f\u5982\u4f55\u66f4\u8fed\uff0c\u5b83\u6240\u8981\u5b9e\u73b0\u7684\u76ee\u6807\uff0c\u4f5c\u8005\u5df2\u7ecf\u544a\u8bc9\u4e86\u6211\u4eec\u3002","title":"Home"},{"location":"NLTK/docs/10-Analyzing-the-Meaning-of-Sentences/10-Analyzing-the-Meaning-of-Sentences/","text":"10. Analyzing the Meaning of Sentences 10. Analyzing the Meaning of Sentences # We have seen how useful it is to harness the power of a computer to process text on a large scale. However, now that we have the machinery of parsers and feature based grammars, can we do anything similarly useful by analyzing the meaning of sentences? The goal of this chapter is to answer the following questions: How can we represent natural language meaning so that a computer can process these representations ? How can we associate meaning representations with an unlimited set of sentences? How can we use programs that connect the meaning representations of sentences to stores of knowledge? Along the way we will learn some formal techniques in the field of logical semantics, and see how these can be used for interrogating databases that store facts about the world.","title":"10 Analyzing the Meaning of Sentences"},{"location":"NLTK/docs/10-Analyzing-the-Meaning-of-Sentences/10-Analyzing-the-Meaning-of-Sentences/#10-analyzing-the-meaning-of-sentences","text":"We have seen how useful it is to harness the power of a computer to process text on a large scale. However, now that we have the machinery of parsers and feature based grammars, can we do anything similarly useful by analyzing the meaning of sentences? The goal of this chapter is to answer the following questions: How can we represent natural language meaning so that a computer can process these representations ? How can we associate meaning representations with an unlimited set of sentences? How can we use programs that connect the meaning representations of sentences to stores of knowledge? Along the way we will learn some formal techniques in the field of logical semantics, and see how these can be used for interrogating databases that store facts about the world.","title":"10. Analyzing the Meaning of Sentences"},{"location":"NLTK/docs/5-Categorizing-and-Tagging-Words/5Categorizing-and-Tagging-Words/","text":"5. Categorizing and Tagging Words # Back in elementary school you learnt the difference between nouns, verbs, adjectives, and adverbs. These \"word classes\" are not just the idle invention of grammarians, but are useful categories for many language processing tasks. As we will see, they arise from simple analysis of the distribution of words in text. The goal of this chapter is to answer the following questions: What are lexical categories and how are they used in natural language processing? What is a good Python data structure for storing words and their categories? How can we automatically tag each word of a text with its word class? Along the way, we'll cover some fundamental techniques in NLP, including sequence labeling , n-gram models, backoff, and evaluation. These techniques are useful in many areas, and tagging gives us a simple context in which to present them. We will also see how tagging is the second step in the typical NLP pipeline, following tokenization . NOTE: NLP pipeline: tokenization->tagging The process of classifying words into their parts of speech and labeling them accordingly is known as part-of-speech tagging , POS-tagging , or simply tagging . Parts of speech are also known as word classes or lexical categories . The collection of tags used for a particular task is known as a tagset . Our emphasis in this chapter is on exploiting tags, and tagging text automatically. NOTE: hierarchy: Parts of speech word classes 1 Using a Tagger # A part-of-speech tagger, or POS-tagger , processes a sequence of words, and attaches a part of speech tag to each word (don't forget to import nltk ): >>> text = word_tokenize(\"And now for something completely different\") >>> nltk.pos_tag(text) [('And', 'CC'), ('now', 'RB'), ('for', 'IN'), ('something', 'NN'), ('completely', 'RB'), ('different', 'JJ')] Here we see that and is CC , a coordinating conjunction; now and completely are RB , or adverbs; for is IN , a preposition; something is NN , a noun; and different is JJ , an adjective. Note NLTK provides documentation for each tag, which can be queried using the tag, e.g. nltk.help.upenn_tagset('RB') , or a regular expression, e.g. nltk.help.upenn_tagset('NN.*') . Some corpora have README files with tagset documentation, see nltk.corpus.???.readme() , substituting in the name of the corpus. Let's look at another example, this time including some homonyms: >>> text = word_tokenize(\"They refuse to permit us to obtain the refuse permit\") >>> nltk.pos_tag(text) [('They', 'PRP'), ('refuse', 'VBP'), ('to', 'TO'), ('permit', 'VB'), ('us', 'PRP'), ('to', 'TO'), ('obtain', 'VB'), ('the', 'DT'), ('refuse', 'NN'), ('permit', 'NN')] Notice that refuse and permit both appear as a present tense verb ( VBP ) and a noun ( NN ). E.g. refUSE is a verb meaning \"deny,\" while REFuse is a noun meaning \"trash\" (i.e. they are not homophones). Thus, we need to know which word is being used in order to pronounce the text correctly. (For this reason, text-to-speech systems usually perform POS-tagging.)","title":"5. Categorizing and Tagging Words"},{"location":"NLTK/docs/5-Categorizing-and-Tagging-Words/5Categorizing-and-Tagging-Words/#5-categorizing-and-tagging-words","text":"Back in elementary school you learnt the difference between nouns, verbs, adjectives, and adverbs. These \"word classes\" are not just the idle invention of grammarians, but are useful categories for many language processing tasks. As we will see, they arise from simple analysis of the distribution of words in text. The goal of this chapter is to answer the following questions: What are lexical categories and how are they used in natural language processing? What is a good Python data structure for storing words and their categories? How can we automatically tag each word of a text with its word class? Along the way, we'll cover some fundamental techniques in NLP, including sequence labeling , n-gram models, backoff, and evaluation. These techniques are useful in many areas, and tagging gives us a simple context in which to present them. We will also see how tagging is the second step in the typical NLP pipeline, following tokenization . NOTE: NLP pipeline: tokenization->tagging The process of classifying words into their parts of speech and labeling them accordingly is known as part-of-speech tagging , POS-tagging , or simply tagging . Parts of speech are also known as word classes or lexical categories . The collection of tags used for a particular task is known as a tagset . Our emphasis in this chapter is on exploiting tags, and tagging text automatically. NOTE: hierarchy: Parts of speech word classes","title":"5. Categorizing and Tagging Words"},{"location":"NLTK/docs/5-Categorizing-and-Tagging-Words/5Categorizing-and-Tagging-Words/#1-using-a-tagger","text":"A part-of-speech tagger, or POS-tagger , processes a sequence of words, and attaches a part of speech tag to each word (don't forget to import nltk ): >>> text = word_tokenize(\"And now for something completely different\") >>> nltk.pos_tag(text) [('And', 'CC'), ('now', 'RB'), ('for', 'IN'), ('something', 'NN'), ('completely', 'RB'), ('different', 'JJ')] Here we see that and is CC , a coordinating conjunction; now and completely are RB , or adverbs; for is IN , a preposition; something is NN , a noun; and different is JJ , an adjective. Note NLTK provides documentation for each tag, which can be queried using the tag, e.g. nltk.help.upenn_tagset('RB') , or a regular expression, e.g. nltk.help.upenn_tagset('NN.*') . Some corpora have README files with tagset documentation, see nltk.corpus.???.readme() , substituting in the name of the corpus. Let's look at another example, this time including some homonyms: >>> text = word_tokenize(\"They refuse to permit us to obtain the refuse permit\") >>> nltk.pos_tag(text) [('They', 'PRP'), ('refuse', 'VBP'), ('to', 'TO'), ('permit', 'VB'), ('us', 'PRP'), ('to', 'TO'), ('obtain', 'VB'), ('the', 'DT'), ('refuse', 'NN'), ('permit', 'NN')] Notice that refuse and permit both appear as a present tense verb ( VBP ) and a noun ( NN ). E.g. refUSE is a verb meaning \"deny,\" while REFuse is a noun meaning \"trash\" (i.e. they are not homophones). Thus, we need to know which word is being used in order to pronounce the text correctly. (For this reason, text-to-speech systems usually perform POS-tagging.)","title":"1  Using a Tagger"},{"location":"NLTK/docs/5-Categorizing-and-Tagging-Words/wikipedia-Part-of-speech-tagging/","text":"Part-of-speech tagging Part-of-speech tagging # In corpus linguistics , part-of-speech tagging ( POS tagging or PoS tagging or POST ), also called grammatical tagging or word-categorydisambiguation , is the process of marking up a word in a text (corpus) as corresponding to a particular part of speech ,[ 1] based on both its definition and its context\u2014i.e., its relationship with adjacent and related words in a phrase , sentence , or paragraph . A simplified form of this is commonly taught to school-age children, in the identification of words as nouns , verbs , adjectives , adverbs , etc. Once performed by hand, POS tagging is now done in the context of computational linguistics , using algorithms which associate discrete terms, as well as hidden parts of speech, in accordance with a set of descriptive tags. POS-tagging algorithms fall into two distinctive groups: rule-based and stochastic. E. Brill's tagger , one of the first and most widely used English POS-taggers, employs rule-based algorithms.","title":"wikipedia Part of speech tagging"},{"location":"NLTK/docs/5-Categorizing-and-Tagging-Words/wikipedia-Part-of-speech-tagging/#part-of-speech-tagging","text":"In corpus linguistics , part-of-speech tagging ( POS tagging or PoS tagging or POST ), also called grammatical tagging or word-categorydisambiguation , is the process of marking up a word in a text (corpus) as corresponding to a particular part of speech ,[ 1] based on both its definition and its context\u2014i.e., its relationship with adjacent and related words in a phrase , sentence , or paragraph . A simplified form of this is commonly taught to school-age children, in the identification of words as nouns , verbs , adjectives , adverbs , etc. Once performed by hand, POS tagging is now done in the context of computational linguistics , using algorithms which associate discrete terms, as well as hidden parts of speech, in accordance with a set of descriptive tags. POS-tagging algorithms fall into two distinctive groups: rule-based and stochastic. E. Brill's tagger , one of the first and most widely used English POS-taggers, employs rule-based algorithms.","title":"Part-of-speech tagging"},{"location":"NLTK/docs/7-Extracting-Information-from-Text/7-Extracting-Information-from-Text/","text":"7. Extracting Information from Text 1 Information Extraction 1.1 Information Extraction Architecture 2 Chunking 2.1 Noun Phrase Chunking 2.2 Tag Patterns 7. Extracting Information from Text # For any given question, it's likely that someone has written the answer down somewhere. The amount of natural language text that is available in electronic form is truly staggering, and is increasing every day. However, the complexity of natural language can make it very difficult to access the information in that text. The state of the art in NLP is still a long way from being able to build general-purpose representations of meaning from unrestricted text. If we instead focus our efforts on a limited set of questions or \"entity relations,\" such as \"where are different facilities located,\" or \"who is employed by what company,\" we can make significant progress. The goal of this chapter is to answer the following questions: How can we build a system that extracts structured data, such as tables, from unstructured text? What are some robust methods for identifying the entities and relationships described in a text? Which corpora are appropriate for this work, and how do we use them for training and evaluating our models? Along the way, we'll apply techniques from the last two chapters to the problems of chunking and named-entity recognition . 1 Information Extraction # Information comes in many shapes and sizes. One important form is structured data , where there is a regular and predictable organization of entities and relationships . For example, we might be interested in the relation between companies and locations. Given a particular company, we would like to be able to identify the locations where it does business; conversely, given a location, we would like to discover which companies do business in that location. If our data is in tabular form, such as the example in 1.1 , then answering these queries is straightforward. Table 1.1 : Locations data OrgName LocationName Omnicom New York DDB Needham New York Kaplan Thaler Group New York BBDO South Atlanta Georgia-Pacific Atlanta Things are more tricky if we try to get similar information out of text. For example, consider the following snippet (from nltk.corpus.ieer, for fileid NYT19980315.0085). (1) The fourth Wells account moving to another agency is the packaged paper-products division of Georgia-Pacific Corp., which arrived at Wells only last fall. Like Hertz and the History Channel, it is also leaving for an Omnicom-owned agency, the BBDO South unit of BBDO Worldwide. BBDO South in Atlanta, which handles corporate advertising for Georgia-Pacific, will assume additional duties for brands like Angel Soft toilet tissue and Sparkle paper towels, said Ken Haldin, a spokesman for Georgia-Pacific in Atlanta. If you read through (1) , you will glean the information required to answer the example question. But how do we get a machine to understand enough about (1) to return the answers in 1.2 ? This is obviously a much harder task. Unlike 1.1 , (1) contains no structure that links organization names with location names. One approach to this problem involves building a very general representation of meaning ( 10. ). In this chapter we take a different approach, deciding in advance that we will only look for very specific kinds of information in text, such as the relation between organizations and locations. Rather than trying to use text like (1) to answer the question directly, we first convert the unstructured data of natural language sentences into the structured data of 1.1 . Then we reap the benefits of powerful query tools such as SQL. This method of getting meaning from text is called Information Extraction . Information Extraction has many applications, including business intelligence, resume harvesting, media analysis, sentiment detection, patent search, and email scanning. A particularly important area of current research involves the attempt to extract structured data out of electronically-available scientific literature, especially in the domain of biology and medicine. 1.1 Information Extraction Architecture # 1.1 shows the architecture for a simple information extraction system . It begins by processing a document using several of the procedures discussed in 3 and 5. : first, the raw text of the document is split into sentences using a sentence segmenter , and each sentence is further subdivided into words using a tokenizer . Next, each sentence is tagged with part-of-speech tags , which will prove very helpful in the next step, named entity detection . In this step, we search for mentions of potentially interesting entities in each sentence. Finally, we use relation detection to search for likely relations between different entities in the text. Figure 1.1 : Simple Pipeline Architecture for an Information Extraction System. This system takes the raw text of a document as its input, and generates a list of (entity, relation, entity) tuples as its output. For example, given a document that indicates that the company Georgia-Pacific is located in Atlanta, it might generate the tuple ([ORG: 'Georgia-Pacific'] 'in' [LOC: 'Atlanta']) . To perform the first three tasks, we can define a simple function that simply connects together NLTK's default sentence segmenter [1], word tokenizer [2], and part-of-speech tagger [3]: >>> def ie_preprocess(document): ... sentences = nltk.sent_tokenize(document) [1] ... sentences = [nltk.word_tokenize(sent) for sent in sentences] [2] ... sentences = [nltk.pos_tag(sent) for sent in sentences] [3] Next, in named entity detection , we segment and label the entities that might participate in interesting relations with one another. Typically, these will be definite noun phrases such as the knights who say \"ni\" , or proper names such as Monty Python . In some tasks it is useful to also consider indefinite nouns or noun chunks, such as every student or cats, and these do not necessarily refer to entities in the same way as definite NP s and proper names. Finally, in relation extraction, we search for specific patterns between pairs of entities that occur near one another in the text, and use those patterns to build tuples recording the relationships between the entities. 2 Chunking # The basic technique we will use for entity detection is chunking , which segments and labels multi-token sequences as illustrated in 2.1 . The smaller boxes show the word-level tokenization and part-of-speech tagging, while the large boxes show higher-level chunking. Each of these larger boxes is called a chunk . Like tokenization, which omits whitespace, chunking usually selects a subset of the tokens. Also like tokenization, the pieces produced by a chunker do not overlap in the source text. Figure 2.1 : Segmentation and Labeling at both the Token and Chunk Levels In this section, we will explore chunking in some depth, beginning with the definition and representation of chunks. We will see regular expression and n-gram approaches to chunking, and will develop and evaluate chunkers using the CoNLL-2000 chunking corpus. We will then return in (5) and 6 to the tasks of named entity recognition and relation extraction. 2.1 Noun Phrase Chunking # We will begin by considering the task of noun phrase chunking , or NP-chunking , where we search for chunks corresponding to individual noun phrases. For example, here is some Wall Street Journal text with NP -chunks marked using brackets: (2) [ The/DT market/NN ] for/IN [ system-management/NN software/NN ] for/IN [ Digital/NNP ] [ 's/POS hardware/NN ] is/VBZ fragmented/JJ enough/RB that/IN [ a/DT giant/NN ] such/JJ as/IN [ Computer/NNP Associates/NNPS ] should/MD do/VB well/RB there/RB ./. As we can see, NP -chunks are often smaller pieces than complete noun phrases. For example, the market for system-management software for Digital's hardware is a single noun phrase (containing two nested noun phrases), but it is captured in NP -chunks by the simpler chunk the market . One of the motivations for this difference is that NP -chunks are defined so as not to contain other NP -chunks. Consequently, any prepositional phrases or subordinate clauses that modify a nominal will not be included in the corresponding NP -chunk, since they almost certainly contain further noun phrases. One of the most useful sources of information for NP -chunking is part-of-speech tags . This is one of the motivations for performing part-of-speech tagging in our information extraction system. We demonstrate this approach using an example sentence that has been part-of-speech tagged in 2.2 . In order to create an NP -chunker, we will first define a chunk grammar , consisting of rules that indicate how sentences should be chunked. In this case, we will define a simple grammar with a single regular-expression rule. This rule says that an NP chunk should be formed whenever the chunker finds an optional determiner ( DT ) followed by any number of adjectives ( JJ ) and then a noun ( NN ). Using this grammar, we create a chunk parser, and test it on our example sentence. The result is a tree, which we can either print, or display graphically. sentence = [(\"the\", \"DT\"), (\"little\", \"JJ\"), (\"yellow\", \"JJ\"), (\"dog\", \"NN\"), (\"barked\", \"VBD\"), (\"at\", \"IN\"), (\"the\", \"DT\"), (\"cat\", \"NN\")] grammar = \"NP: {<DT>?<JJ>*<NN>}\" cp = nltk.RegexpParser(grammar) result = cp.parse(sentence) print(result) (S (NP the/DT little/JJ yellow/JJ dog/NN) barked/VBD at/IN (NP the/DT cat/NN)) result.draw() Example 2.2 (code_chunkex.py) : Figure 2.2 : Example of a Simple Regular Expression Based NP Chunker. NOTE: In fact, this is a parse tree. 2.2 Tag Patterns # The rules that make up a chunk grammar use tag patterns to describe sequences of tagged words. A tag pattern is a sequence of part-of-speech tags delimited using angle brackets, e.g. ?* . Tag patterns are similar to regular expression patterns ( 3.4 ). Now, consider the following noun phrases from the Wall Street Journal: another/DT sharp/JJ dive/NN trade/NN figures/NNS any/DT new/JJ policy/NN measures/NNS earlier/JJR stages/NNS Panamanian/JJ dictator/NN Manuel/NNP Noriega/NNP We can match these noun phrases using a slight refinement of the first tag pattern above, i.e. ?*+ . This will chunk any sequence of tokens beginning with an optional determiner, followed by zero or more adjectives of any type (including relative adjectives like earlier/JJR ), followed by one or more nouns of any type. However, it is easy to find many more complicated examples which this rule will not cover: his/PRP$ Mansion/NNP House/NNP speech/NN the/DT price/NN cutting/VBG 3/CD %/NN to/TO 4/CD %/NN more/JJR than/IN 10/CD %/NN the/DT fastest/JJS developing/VBG trends/NNS 's/POS skill/NN Note Your Turn: Try to come up with tag patterns to cover these cases. Test them using the graphical interface nltk.app.chunkparser() . Continue to refine your tag patterns with the help of the feedback given by this tool.","title":"7 Extracting Information from Text"},{"location":"NLTK/docs/7-Extracting-Information-from-Text/7-Extracting-Information-from-Text/#7-extracting-information-from-text","text":"For any given question, it's likely that someone has written the answer down somewhere. The amount of natural language text that is available in electronic form is truly staggering, and is increasing every day. However, the complexity of natural language can make it very difficult to access the information in that text. The state of the art in NLP is still a long way from being able to build general-purpose representations of meaning from unrestricted text. If we instead focus our efforts on a limited set of questions or \"entity relations,\" such as \"where are different facilities located,\" or \"who is employed by what company,\" we can make significant progress. The goal of this chapter is to answer the following questions: How can we build a system that extracts structured data, such as tables, from unstructured text? What are some robust methods for identifying the entities and relationships described in a text? Which corpora are appropriate for this work, and how do we use them for training and evaluating our models? Along the way, we'll apply techniques from the last two chapters to the problems of chunking and named-entity recognition .","title":"7. Extracting Information from Text"},{"location":"NLTK/docs/7-Extracting-Information-from-Text/7-Extracting-Information-from-Text/#1-information-extraction","text":"Information comes in many shapes and sizes. One important form is structured data , where there is a regular and predictable organization of entities and relationships . For example, we might be interested in the relation between companies and locations. Given a particular company, we would like to be able to identify the locations where it does business; conversely, given a location, we would like to discover which companies do business in that location. If our data is in tabular form, such as the example in 1.1 , then answering these queries is straightforward. Table 1.1 : Locations data OrgName LocationName Omnicom New York DDB Needham New York Kaplan Thaler Group New York BBDO South Atlanta Georgia-Pacific Atlanta Things are more tricky if we try to get similar information out of text. For example, consider the following snippet (from nltk.corpus.ieer, for fileid NYT19980315.0085). (1) The fourth Wells account moving to another agency is the packaged paper-products division of Georgia-Pacific Corp., which arrived at Wells only last fall. Like Hertz and the History Channel, it is also leaving for an Omnicom-owned agency, the BBDO South unit of BBDO Worldwide. BBDO South in Atlanta, which handles corporate advertising for Georgia-Pacific, will assume additional duties for brands like Angel Soft toilet tissue and Sparkle paper towels, said Ken Haldin, a spokesman for Georgia-Pacific in Atlanta. If you read through (1) , you will glean the information required to answer the example question. But how do we get a machine to understand enough about (1) to return the answers in 1.2 ? This is obviously a much harder task. Unlike 1.1 , (1) contains no structure that links organization names with location names. One approach to this problem involves building a very general representation of meaning ( 10. ). In this chapter we take a different approach, deciding in advance that we will only look for very specific kinds of information in text, such as the relation between organizations and locations. Rather than trying to use text like (1) to answer the question directly, we first convert the unstructured data of natural language sentences into the structured data of 1.1 . Then we reap the benefits of powerful query tools such as SQL. This method of getting meaning from text is called Information Extraction . Information Extraction has many applications, including business intelligence, resume harvesting, media analysis, sentiment detection, patent search, and email scanning. A particularly important area of current research involves the attempt to extract structured data out of electronically-available scientific literature, especially in the domain of biology and medicine.","title":"1  Information Extraction"},{"location":"NLTK/docs/7-Extracting-Information-from-Text/7-Extracting-Information-from-Text/#11-information-extraction-architecture","text":"1.1 shows the architecture for a simple information extraction system . It begins by processing a document using several of the procedures discussed in 3 and 5. : first, the raw text of the document is split into sentences using a sentence segmenter , and each sentence is further subdivided into words using a tokenizer . Next, each sentence is tagged with part-of-speech tags , which will prove very helpful in the next step, named entity detection . In this step, we search for mentions of potentially interesting entities in each sentence. Finally, we use relation detection to search for likely relations between different entities in the text. Figure 1.1 : Simple Pipeline Architecture for an Information Extraction System. This system takes the raw text of a document as its input, and generates a list of (entity, relation, entity) tuples as its output. For example, given a document that indicates that the company Georgia-Pacific is located in Atlanta, it might generate the tuple ([ORG: 'Georgia-Pacific'] 'in' [LOC: 'Atlanta']) . To perform the first three tasks, we can define a simple function that simply connects together NLTK's default sentence segmenter [1], word tokenizer [2], and part-of-speech tagger [3]: >>> def ie_preprocess(document): ... sentences = nltk.sent_tokenize(document) [1] ... sentences = [nltk.word_tokenize(sent) for sent in sentences] [2] ... sentences = [nltk.pos_tag(sent) for sent in sentences] [3] Next, in named entity detection , we segment and label the entities that might participate in interesting relations with one another. Typically, these will be definite noun phrases such as the knights who say \"ni\" , or proper names such as Monty Python . In some tasks it is useful to also consider indefinite nouns or noun chunks, such as every student or cats, and these do not necessarily refer to entities in the same way as definite NP s and proper names. Finally, in relation extraction, we search for specific patterns between pairs of entities that occur near one another in the text, and use those patterns to build tuples recording the relationships between the entities.","title":"1.1  Information Extraction Architecture"},{"location":"NLTK/docs/7-Extracting-Information-from-Text/7-Extracting-Information-from-Text/#2-chunking","text":"The basic technique we will use for entity detection is chunking , which segments and labels multi-token sequences as illustrated in 2.1 . The smaller boxes show the word-level tokenization and part-of-speech tagging, while the large boxes show higher-level chunking. Each of these larger boxes is called a chunk . Like tokenization, which omits whitespace, chunking usually selects a subset of the tokens. Also like tokenization, the pieces produced by a chunker do not overlap in the source text. Figure 2.1 : Segmentation and Labeling at both the Token and Chunk Levels In this section, we will explore chunking in some depth, beginning with the definition and representation of chunks. We will see regular expression and n-gram approaches to chunking, and will develop and evaluate chunkers using the CoNLL-2000 chunking corpus. We will then return in (5) and 6 to the tasks of named entity recognition and relation extraction.","title":"2  Chunking"},{"location":"NLTK/docs/7-Extracting-Information-from-Text/7-Extracting-Information-from-Text/#21-noun-phrase-chunking","text":"We will begin by considering the task of noun phrase chunking , or NP-chunking , where we search for chunks corresponding to individual noun phrases. For example, here is some Wall Street Journal text with NP -chunks marked using brackets: (2) [ The/DT market/NN ] for/IN [ system-management/NN software/NN ] for/IN [ Digital/NNP ] [ 's/POS hardware/NN ] is/VBZ fragmented/JJ enough/RB that/IN [ a/DT giant/NN ] such/JJ as/IN [ Computer/NNP Associates/NNPS ] should/MD do/VB well/RB there/RB ./. As we can see, NP -chunks are often smaller pieces than complete noun phrases. For example, the market for system-management software for Digital's hardware is a single noun phrase (containing two nested noun phrases), but it is captured in NP -chunks by the simpler chunk the market . One of the motivations for this difference is that NP -chunks are defined so as not to contain other NP -chunks. Consequently, any prepositional phrases or subordinate clauses that modify a nominal will not be included in the corresponding NP -chunk, since they almost certainly contain further noun phrases. One of the most useful sources of information for NP -chunking is part-of-speech tags . This is one of the motivations for performing part-of-speech tagging in our information extraction system. We demonstrate this approach using an example sentence that has been part-of-speech tagged in 2.2 . In order to create an NP -chunker, we will first define a chunk grammar , consisting of rules that indicate how sentences should be chunked. In this case, we will define a simple grammar with a single regular-expression rule. This rule says that an NP chunk should be formed whenever the chunker finds an optional determiner ( DT ) followed by any number of adjectives ( JJ ) and then a noun ( NN ). Using this grammar, we create a chunk parser, and test it on our example sentence. The result is a tree, which we can either print, or display graphically. sentence = [(\"the\", \"DT\"), (\"little\", \"JJ\"), (\"yellow\", \"JJ\"), (\"dog\", \"NN\"), (\"barked\", \"VBD\"), (\"at\", \"IN\"), (\"the\", \"DT\"), (\"cat\", \"NN\")] grammar = \"NP: {<DT>?<JJ>*<NN>}\" cp = nltk.RegexpParser(grammar) result = cp.parse(sentence) print(result) (S (NP the/DT little/JJ yellow/JJ dog/NN) barked/VBD at/IN (NP the/DT cat/NN)) result.draw() Example 2.2 (code_chunkex.py) : Figure 2.2 : Example of a Simple Regular Expression Based NP Chunker. NOTE: In fact, this is a parse tree.","title":"2.1  Noun Phrase Chunking"},{"location":"NLTK/docs/7-Extracting-Information-from-Text/7-Extracting-Information-from-Text/#22-tag-patterns","text":"The rules that make up a chunk grammar use tag patterns to describe sequences of tagged words. A tag pattern is a sequence of part-of-speech tags delimited using angle brackets, e.g. ?* . Tag patterns are similar to regular expression patterns ( 3.4 ). Now, consider the following noun phrases from the Wall Street Journal: another/DT sharp/JJ dive/NN trade/NN figures/NNS any/DT new/JJ policy/NN measures/NNS earlier/JJR stages/NNS Panamanian/JJ dictator/NN Manuel/NNP Noriega/NNP We can match these noun phrases using a slight refinement of the first tag pattern above, i.e. ?*+ . This will chunk any sequence of tokens beginning with an optional determiner, followed by zero or more adjectives of any type (including relative adjectives like earlier/JJR ), followed by one or more nouns of any type. However, it is easy to find many more complicated examples which this rule will not cover: his/PRP$ Mansion/NNP House/NNP speech/NN the/DT price/NN cutting/VBG 3/CD %/NN to/TO 4/CD %/NN more/JJR than/IN 10/CD %/NN the/DT fastest/JJS developing/VBG trends/NNS 's/POS skill/NN Note Your Turn: Try to come up with tag patterns to cover these cases. Test them using the graphical interface nltk.app.chunkparser() . Continue to refine your tag patterns with the help of the feedback given by this tool.","title":"2.2  Tag Patterns"},{"location":"NLTK/docs/8-Analyzing-Sentence-Structure/8-Analyzing-Sentence-Structure/","text":"8. Analyzing Sentence Structure 1 Some Grammatical Dilemmas 1.1 Linguistic Data and Unlimited Possibilities 1.2 Ubiquitous Ambiguity 2 What's the Use of Syntax? 3 Context Free Grammar 3.1 A Simple Grammar 3.2 Writing Your Own Grammars 3.3 Recursion in Syntactic Structure 4 Parsing With Context Free Grammar 4.1 Recursive Descent Parsing 4.2 Shift-Reduce Parsing 4.3 The Left-Corner Parser 4.4 Well-Formed Substring Tables 5 Dependencies and Dependency Grammar 6 Grammar Development 6.1 Treebanks and Grammars 6.2 Pernicious Ambiguity 6.3 Weighted Grammar 7 Summary 8. Analyzing Sentence Structure # Earlier chapters focused on words: how to identify them, analyze their structure, assign them to lexical categories, and access their meanings. We have also seen how to identify patterns in word sequences or n-grams. However, these methods only scratch the surface of the complex constraints that govern sentences. We need a way to deal with the ambiguity that natural language is famous for. We also need to be able to cope with the fact that there are an unlimited number of possible sentences, and we can only write finite programs to analyze their structures and discover their meanings. NOTE: As far as grammar is concerned, formal language is much simpler than natural language The goal of this chapter is to answer the following questions: How can we use a formal grammar to describe the structure of an unlimited set of sentences? How do we represent the structure of sentences using syntax trees ? How do parsers analyze a sentence and automatically build a syntax tree ? Along the way, we will cover the fundamentals of English syntax, and see that there are systematic aspects of meaning that are much easier to capture once we have identified the structure of sentences. NOTE: My GitHub project automata-and-formal-language summary the knowledge related to formal language. 1 Some Grammatical Dilemmas # 1.1 Linguistic Data and Unlimited Possibilities # In this chapter, we will adopt the formal framework of \"generative grammar\", in which a \"language\" is considered to be nothing more than an enormous collection of all grammatical sentences, and a grammar is a formal notation that can be used for \"generating\" the members of this set. Grammars use recursive productions of the form S \u2192 S and S , as we will explore in 3 . In 10. we will extend this, to automatically build up the meaning of a sentence out of the meanings of its parts. 1.2 Ubiquitous Ambiguity # A well-known example of ambiguity is shown in (2) , from the Groucho Marx movie, Animal Crackers (1930): (2) While hunting in Africa, I shot an elephant in my pajamas. How he got into my pajamas, I don't know. Let's take a closer look at the ambiguity in the phrase: I shot an elephant in my pajamas . First we need to define a simple grammar: groucho_grammar = nltk.CFG.fromstring(\"\"\"S -> NP VP PP -> P NP NP -> Det N | Det N PP | 'I' VP -> V NP | VP PP Det -> 'an' | 'my' N -> 'elephant' | 'pajamas' V -> 'shot' P -> 'in' \"\"\") NOTE: What dose NP, VP mean? see 2 What's the Use of Syntax? 3 Context Free Grammar This grammar permits the sentence to be analyzed in two ways, depending on whether the prepositional phrase in my pajamas describes the elephant or the shooting event. sent = ['I', 'shot', 'an', 'elephant', 'in', 'my', 'pajamas'] parser = nltk.ChartParser(groucho_grammar) for tree in parser.parse(sent): print(tree) The program produces two bracketed structures, which we can depict as trees, as shown in (3b) : (3) a. b. Notice that there's no ambiguity concerning the meaning of any of the words; This chapter presents grammars and parsing, as the formal and computational methods for investigating and modeling the linguistic phenomena we have been discussing. As we shall see, patterns of well-formedness and ill-formedness in a sequence of words can be understood with respect to the phrase structure and dependencies. We can develop formal models of these structures using grammars and parsers. As before, a key motivation is natural language understanding . How much more of the meaning of a text can we access when we can reliably recognize the linguistic structures it contains? Having read in a text, can a program \"understand\" it enough to be able to answer simple questions about \"what happened\" or \"who did what to whom\"? Also as before, we will develop simple programs to process annotated corpora and perform useful tasks. 2 What's the Use of Syntax? # In 2.2 , we have added grammatical category labels to the words we saw in the earlier figure. The labels NP , VP , and PP stand for noun phrase , verb phrase and prepositional phrase respectively. Figure 2.2 : Substitution of Word Sequences Plus Grammatical Categories: This diagram reproduces 2.1 along with grammatical categories corresponding to noun phrases ( NP ), verb phrases ( VP ), prepositional phrases ( PP ), and nominals ( Nom ). If we now strip out the words apart from the topmost row, add an S node, and flip the figure over, we end up with a standard phrase structure tree, shown in (8) . Each node in this tree (including the words) is called a constituent . The immediate constituents of S are NP and VP . (8) As we will see in the next section, a grammar specifies how the sentence can be subdivided into its immediate constituents, and how these can be further subdivided until we reach the level of individual words. Note As we saw in 1 , sentences can have arbitrary length. Consequently, phrase structure trees can have arbitrary depth . The cascaded chunk parsers we saw in 4 can only produce structures of bounded depth, so chunking methods aren't applicable here. 3 Context Free Grammar # 3.1 A Simple Grammar # Let's start off by looking at a simple context-free grammar. By convention, the left-hand-side of the first production is the start-symbol of the grammar, typically S , and all well-formed trees must have this symbol as their root label. In NLTK, context-free grammars are defined in the nltk.grammar module. In 3.1 we define a grammar and show how to parse a simple sentence admitted by the grammar. grammar1 = nltk.CFG.fromstring(\"\"\" S -> NP VP VP -> V NP | V NP PP PP -> P NP V -> \"saw\" | \"ate\" | \"walked\" NP -> \"John\" | \"Mary\" | \"Bob\" | Det N | Det N PP Det -> \"a\" | \"an\" | \"the\" | \"my\" N -> \"man\" | \"dog\" | \"cat\" | \"telescope\" | \"park\" P -> \"in\" | \"on\" | \"by\" | \"with\" \"\"\") sent = \"Mary saw Bob\".split() rd_parser = nltk.RecursiveDescentParser(grammar1) for tree in rd_parser.parse(sent): print(tree) The grammar in 3.1 contains productions involving various syntactic categories, as laid out in 3.1 . Table 3.1 : Syntactic Categories Symbol Meaning Example S sentence the man walked NP noun phrase a dog VP verb phrase saw a park PP prepositional phrase with a telescope Det determiner the N noun dog V verb walked P preposition in A production like VP -> V NP | V NP PP has a disjunction on the righthand side, shown by the | and is an abbreviation for the two productions VP -> V NP and VP -> V NP PP . Figure 3.2 : Recursive Descent Parser Demo: This tool allows you to watch the operation of a recursive descent parser as it grows the parse tree and matches it against the input words. Note Your Turn: Try developing a simple grammar of your own, using the recursive descent parser application, nltk.app.rdparser() , shown in 3.2 . It comes already loaded with a sample grammar, but you can edit this as you please (using the Edit menu). Change the grammar, and the sentence to be parsed, and run the parser using the autostep button. If we parse the sentence The dog saw a man in the park using the grammar shown in 3.1 , we end up with two trees, similar to those we saw for (3b) : a. b. 3.2 Writing Your Own Grammars # If you are interested in experimenting with writing CFGs, you will find it helpful to create and edit your grammar in a text file, say mygrammar.cfg . You can then load it into NLTK and parse with it as follows: grammar1 = nltk.data.load('file:mygrammar.cfg') sent = \"Mary saw Bob\".split() rd_parser = nltk.RecursiveDescentParser(grammar1) for tree in rd_parser.parse(sent): print(tree) 3.3 Recursion in Syntactic Structure # A grammar is said to be recursive if a category occurring on the left hand side of a production also appears on the righthand side of a production, as illustrated in 3.3 . The production Nom -> Adj Nom (where Nom is the category of nominals) involves direct recursion on the category Nom , whereas indirect recursion on S arises from the combination of two productions, namely S -> NP VP and VP -> V S . grammar2 = nltk.CFG.fromstring(\"\"\" S -> NP VP NP -> Det Nom | PropN Nom -> Adj Nom | N VP -> V Adj | V NP | V S | V NP PP PP -> P NP PropN -> 'Buster' | 'Chatterer' | 'Joe' Det -> 'the' | 'a' N -> 'bear' | 'squirrel' | 'tree' | 'fish' | 'log' Adj -> 'angry' | 'frightened' | 'little' | 'tall' V -> 'chased' | 'saw' | 'said' | 'thought' | 'was' | 'put' P -> 'on' \"\"\") Example 3.3 (code_cfg2.py) : Figure 3.3 : A Recursive Context-Free Grammar To see how recursion arises from this grammar, consider the following trees. (10a) involves nested nominal phrases, while (10b) contains nested sentences. (10) a. b. We've only illustrated two levels of recursion here, but there's no upper limit on the depth. You can experiment with parsing sentences that involve more deeply nested structures. Beware that the RecursiveDescentParser is unable to handle left-recursive productions of the form X -> X Y ; we will return to this in 4 . 4 Parsing With Context Free Grammar # A parser processes input sentences according to the productions of a grammar, and builds one or more constituent structures that conform to the grammar. A grammar is a declarative specification of well-formedness \u2014 it is actually just a string, not a program. A parser is a procedural interpretation of the grammar. It searches through the space of trees licensed by a grammar to find one that has the required sentence along its fringe. A parser permits a grammar to be evaluated against a collection of test sentences, helping linguists to discover mistakes in their grammatical analysis. A parser can serve as a model of psycholinguistic processing, helping to explain the difficulties that humans have with processing certain syntactic constructions. Many natural language applications involve parsing at some point; for example, we would expect the natural language questions submitted to a question-answering system to undergo parsing as an initial step.In this section we see two simple parsing algorithms, a top-down method called recursive descent parsing, and a bottom-up method called shift-reduce parsing. We also see some more sophisticated algorithms, a top-down method with bottom-up filtering called left-corner parsing , and a dynamic programming technique called chart parsing . 4.1 Recursive Descent Parsing # 4.2 Shift-Reduce Parsing # 4.3 The Left-Corner Parser # 4.4 Well-Formed Substring Tables # 5 Dependencies and Dependency Grammar # 6 Grammar Development # Parsing builds trees over sentences, according to a phrase structure grammar. Now, all the examples we gave above only involved toy grammars containing a handful of productions. What happens if we try to scale up this approach to deal with realistic corpora of language? In this section we will see how to access treebanks, and look at the challenge of developing broad-coverage grammars. 6.1 Treebanks and Grammars # 6.2 Pernicious Ambiguity # 6.3 Weighted Grammar # 7 Summary #","title":"8 Analyzing Sentence Structure"},{"location":"NLTK/docs/8-Analyzing-Sentence-Structure/8-Analyzing-Sentence-Structure/#8-analyzing-sentence-structure","text":"Earlier chapters focused on words: how to identify them, analyze their structure, assign them to lexical categories, and access their meanings. We have also seen how to identify patterns in word sequences or n-grams. However, these methods only scratch the surface of the complex constraints that govern sentences. We need a way to deal with the ambiguity that natural language is famous for. We also need to be able to cope with the fact that there are an unlimited number of possible sentences, and we can only write finite programs to analyze their structures and discover their meanings. NOTE: As far as grammar is concerned, formal language is much simpler than natural language The goal of this chapter is to answer the following questions: How can we use a formal grammar to describe the structure of an unlimited set of sentences? How do we represent the structure of sentences using syntax trees ? How do parsers analyze a sentence and automatically build a syntax tree ? Along the way, we will cover the fundamentals of English syntax, and see that there are systematic aspects of meaning that are much easier to capture once we have identified the structure of sentences. NOTE: My GitHub project automata-and-formal-language summary the knowledge related to formal language.","title":"8. Analyzing Sentence Structure"},{"location":"NLTK/docs/8-Analyzing-Sentence-Structure/8-Analyzing-Sentence-Structure/#1-some-grammatical-dilemmas","text":"","title":"1  Some Grammatical Dilemmas"},{"location":"NLTK/docs/8-Analyzing-Sentence-Structure/8-Analyzing-Sentence-Structure/#11-linguistic-data-and-unlimited-possibilities","text":"In this chapter, we will adopt the formal framework of \"generative grammar\", in which a \"language\" is considered to be nothing more than an enormous collection of all grammatical sentences, and a grammar is a formal notation that can be used for \"generating\" the members of this set. Grammars use recursive productions of the form S \u2192 S and S , as we will explore in 3 . In 10. we will extend this, to automatically build up the meaning of a sentence out of the meanings of its parts.","title":"1.1  Linguistic Data and Unlimited Possibilities"},{"location":"NLTK/docs/8-Analyzing-Sentence-Structure/8-Analyzing-Sentence-Structure/#12-ubiquitous-ambiguity","text":"A well-known example of ambiguity is shown in (2) , from the Groucho Marx movie, Animal Crackers (1930): (2) While hunting in Africa, I shot an elephant in my pajamas. How he got into my pajamas, I don't know. Let's take a closer look at the ambiguity in the phrase: I shot an elephant in my pajamas . First we need to define a simple grammar: groucho_grammar = nltk.CFG.fromstring(\"\"\"S -> NP VP PP -> P NP NP -> Det N | Det N PP | 'I' VP -> V NP | VP PP Det -> 'an' | 'my' N -> 'elephant' | 'pajamas' V -> 'shot' P -> 'in' \"\"\") NOTE: What dose NP, VP mean? see 2 What's the Use of Syntax? 3 Context Free Grammar This grammar permits the sentence to be analyzed in two ways, depending on whether the prepositional phrase in my pajamas describes the elephant or the shooting event. sent = ['I', 'shot', 'an', 'elephant', 'in', 'my', 'pajamas'] parser = nltk.ChartParser(groucho_grammar) for tree in parser.parse(sent): print(tree) The program produces two bracketed structures, which we can depict as trees, as shown in (3b) : (3) a. b. Notice that there's no ambiguity concerning the meaning of any of the words; This chapter presents grammars and parsing, as the formal and computational methods for investigating and modeling the linguistic phenomena we have been discussing. As we shall see, patterns of well-formedness and ill-formedness in a sequence of words can be understood with respect to the phrase structure and dependencies. We can develop formal models of these structures using grammars and parsers. As before, a key motivation is natural language understanding . How much more of the meaning of a text can we access when we can reliably recognize the linguistic structures it contains? Having read in a text, can a program \"understand\" it enough to be able to answer simple questions about \"what happened\" or \"who did what to whom\"? Also as before, we will develop simple programs to process annotated corpora and perform useful tasks.","title":"1.2  Ubiquitous Ambiguity"},{"location":"NLTK/docs/8-Analyzing-Sentence-Structure/8-Analyzing-Sentence-Structure/#2-whats-the-use-of-syntax","text":"In 2.2 , we have added grammatical category labels to the words we saw in the earlier figure. The labels NP , VP , and PP stand for noun phrase , verb phrase and prepositional phrase respectively. Figure 2.2 : Substitution of Word Sequences Plus Grammatical Categories: This diagram reproduces 2.1 along with grammatical categories corresponding to noun phrases ( NP ), verb phrases ( VP ), prepositional phrases ( PP ), and nominals ( Nom ). If we now strip out the words apart from the topmost row, add an S node, and flip the figure over, we end up with a standard phrase structure tree, shown in (8) . Each node in this tree (including the words) is called a constituent . The immediate constituents of S are NP and VP . (8) As we will see in the next section, a grammar specifies how the sentence can be subdivided into its immediate constituents, and how these can be further subdivided until we reach the level of individual words. Note As we saw in 1 , sentences can have arbitrary length. Consequently, phrase structure trees can have arbitrary depth . The cascaded chunk parsers we saw in 4 can only produce structures of bounded depth, so chunking methods aren't applicable here.","title":"2  What's the Use of Syntax?"},{"location":"NLTK/docs/8-Analyzing-Sentence-Structure/8-Analyzing-Sentence-Structure/#3-context-free-grammar","text":"","title":"3  Context Free Grammar"},{"location":"NLTK/docs/8-Analyzing-Sentence-Structure/8-Analyzing-Sentence-Structure/#31-a-simple-grammar","text":"Let's start off by looking at a simple context-free grammar. By convention, the left-hand-side of the first production is the start-symbol of the grammar, typically S , and all well-formed trees must have this symbol as their root label. In NLTK, context-free grammars are defined in the nltk.grammar module. In 3.1 we define a grammar and show how to parse a simple sentence admitted by the grammar. grammar1 = nltk.CFG.fromstring(\"\"\" S -> NP VP VP -> V NP | V NP PP PP -> P NP V -> \"saw\" | \"ate\" | \"walked\" NP -> \"John\" | \"Mary\" | \"Bob\" | Det N | Det N PP Det -> \"a\" | \"an\" | \"the\" | \"my\" N -> \"man\" | \"dog\" | \"cat\" | \"telescope\" | \"park\" P -> \"in\" | \"on\" | \"by\" | \"with\" \"\"\") sent = \"Mary saw Bob\".split() rd_parser = nltk.RecursiveDescentParser(grammar1) for tree in rd_parser.parse(sent): print(tree) The grammar in 3.1 contains productions involving various syntactic categories, as laid out in 3.1 . Table 3.1 : Syntactic Categories Symbol Meaning Example S sentence the man walked NP noun phrase a dog VP verb phrase saw a park PP prepositional phrase with a telescope Det determiner the N noun dog V verb walked P preposition in A production like VP -> V NP | V NP PP has a disjunction on the righthand side, shown by the | and is an abbreviation for the two productions VP -> V NP and VP -> V NP PP . Figure 3.2 : Recursive Descent Parser Demo: This tool allows you to watch the operation of a recursive descent parser as it grows the parse tree and matches it against the input words. Note Your Turn: Try developing a simple grammar of your own, using the recursive descent parser application, nltk.app.rdparser() , shown in 3.2 . It comes already loaded with a sample grammar, but you can edit this as you please (using the Edit menu). Change the grammar, and the sentence to be parsed, and run the parser using the autostep button. If we parse the sentence The dog saw a man in the park using the grammar shown in 3.1 , we end up with two trees, similar to those we saw for (3b) : a. b.","title":"3.1  A Simple Grammar"},{"location":"NLTK/docs/8-Analyzing-Sentence-Structure/8-Analyzing-Sentence-Structure/#32-writing-your-own-grammars","text":"If you are interested in experimenting with writing CFGs, you will find it helpful to create and edit your grammar in a text file, say mygrammar.cfg . You can then load it into NLTK and parse with it as follows: grammar1 = nltk.data.load('file:mygrammar.cfg') sent = \"Mary saw Bob\".split() rd_parser = nltk.RecursiveDescentParser(grammar1) for tree in rd_parser.parse(sent): print(tree)","title":"3.2  Writing Your Own Grammars"},{"location":"NLTK/docs/8-Analyzing-Sentence-Structure/8-Analyzing-Sentence-Structure/#33-recursion-in-syntactic-structure","text":"A grammar is said to be recursive if a category occurring on the left hand side of a production also appears on the righthand side of a production, as illustrated in 3.3 . The production Nom -> Adj Nom (where Nom is the category of nominals) involves direct recursion on the category Nom , whereas indirect recursion on S arises from the combination of two productions, namely S -> NP VP and VP -> V S . grammar2 = nltk.CFG.fromstring(\"\"\" S -> NP VP NP -> Det Nom | PropN Nom -> Adj Nom | N VP -> V Adj | V NP | V S | V NP PP PP -> P NP PropN -> 'Buster' | 'Chatterer' | 'Joe' Det -> 'the' | 'a' N -> 'bear' | 'squirrel' | 'tree' | 'fish' | 'log' Adj -> 'angry' | 'frightened' | 'little' | 'tall' V -> 'chased' | 'saw' | 'said' | 'thought' | 'was' | 'put' P -> 'on' \"\"\") Example 3.3 (code_cfg2.py) : Figure 3.3 : A Recursive Context-Free Grammar To see how recursion arises from this grammar, consider the following trees. (10a) involves nested nominal phrases, while (10b) contains nested sentences. (10) a. b. We've only illustrated two levels of recursion here, but there's no upper limit on the depth. You can experiment with parsing sentences that involve more deeply nested structures. Beware that the RecursiveDescentParser is unable to handle left-recursive productions of the form X -> X Y ; we will return to this in 4 .","title":"3.3  Recursion in Syntactic Structure"},{"location":"NLTK/docs/8-Analyzing-Sentence-Structure/8-Analyzing-Sentence-Structure/#4-parsing-with-context-free-grammar","text":"A parser processes input sentences according to the productions of a grammar, and builds one or more constituent structures that conform to the grammar. A grammar is a declarative specification of well-formedness \u2014 it is actually just a string, not a program. A parser is a procedural interpretation of the grammar. It searches through the space of trees licensed by a grammar to find one that has the required sentence along its fringe. A parser permits a grammar to be evaluated against a collection of test sentences, helping linguists to discover mistakes in their grammatical analysis. A parser can serve as a model of psycholinguistic processing, helping to explain the difficulties that humans have with processing certain syntactic constructions. Many natural language applications involve parsing at some point; for example, we would expect the natural language questions submitted to a question-answering system to undergo parsing as an initial step.In this section we see two simple parsing algorithms, a top-down method called recursive descent parsing, and a bottom-up method called shift-reduce parsing. We also see some more sophisticated algorithms, a top-down method with bottom-up filtering called left-corner parsing , and a dynamic programming technique called chart parsing .","title":"4  Parsing With Context Free Grammar"},{"location":"NLTK/docs/8-Analyzing-Sentence-Structure/8-Analyzing-Sentence-Structure/#41-recursive-descent-parsing","text":"","title":"4.1  Recursive Descent Parsing"},{"location":"NLTK/docs/8-Analyzing-Sentence-Structure/8-Analyzing-Sentence-Structure/#42-shift-reduce-parsing","text":"","title":"4.2  Shift-Reduce Parsing"},{"location":"NLTK/docs/8-Analyzing-Sentence-Structure/8-Analyzing-Sentence-Structure/#43-the-left-corner-parser","text":"","title":"4.3  The Left-Corner Parser"},{"location":"NLTK/docs/8-Analyzing-Sentence-Structure/8-Analyzing-Sentence-Structure/#44-well-formed-substring-tables","text":"","title":"4.4  Well-Formed Substring Tables"},{"location":"NLTK/docs/8-Analyzing-Sentence-Structure/8-Analyzing-Sentence-Structure/#5-dependencies-and-dependency-grammar","text":"","title":"5  Dependencies and Dependency Grammar"},{"location":"NLTK/docs/8-Analyzing-Sentence-Structure/8-Analyzing-Sentence-Structure/#6-grammar-development","text":"Parsing builds trees over sentences, according to a phrase structure grammar. Now, all the examples we gave above only involved toy grammars containing a handful of productions. What happens if we try to scale up this approach to deal with realistic corpora of language? In this section we will see how to access treebanks, and look at the challenge of developing broad-coverage grammars.","title":"6  Grammar Development"},{"location":"NLTK/docs/8-Analyzing-Sentence-Structure/8-Analyzing-Sentence-Structure/#61-treebanks-and-grammars","text":"","title":"6.1  Treebanks and Grammars"},{"location":"NLTK/docs/8-Analyzing-Sentence-Structure/8-Analyzing-Sentence-Structure/#62-pernicious-ambiguity","text":"","title":"6.2  Pernicious Ambiguity"},{"location":"NLTK/docs/8-Analyzing-Sentence-Structure/8-Analyzing-Sentence-Structure/#63-weighted-grammar","text":"","title":"6.3  Weighted Grammar"},{"location":"NLTK/docs/8-Analyzing-Sentence-Structure/8-Analyzing-Sentence-Structure/#7-summary","text":"","title":"7  Summary"},{"location":"NLTK/implementation/parse/","text":"parser\u7684\u5b9e\u73b0\u5206\u6790 # tree.py probability.py grammar.py parse \u529f\u80fd\u5206\u6790 # \u6839\u636e\u7528\u6237\u6307\u5b9a\u7684grammar\uff0c\u6784\u9020parser\u3002 \u652f\u6301\u7684\u8bed\u6cd5 # CFG PCFG( probabilistic context free grammar ) \u9996\u5148\u9700\u8981\u8003\u8651\u7684\u5982\u4f55\u6765\u8868\u793a CFG ? \u7531 grammar.py \u5b9e\u73b0\u3002 Nonterminal # class Nonterminal A non-terminal symbol for a context free grammar, immutable and hashable class FeatStructNonterminal Productions # class Production class DependencyProduction(Production) A dependency grammar production class ProbabilisticProduction Grammars # class CFG A context-free grammar. A grammar consists of a start state and a set of productions. class FeatureGrammar(CFG) A dependency grammar production class DependencyGrammar class ProbabilisticDependencyGrammar grammar # class RecursiveDescentParser(ParserI) # class TransitionParser(ParserI) # class BottomUpProbabilisticChartParser(ParserI) # class MaltParser(ParserI) # class GenericStanfordParser(ParserI) # class ViterbiParser(ParserI) # class ChartParser(ParserI) # class GenericCoreNLPParser(ParserI, TokenizerI, TaggerI) # class BllipParser(ParserI) # class ShiftReduceParser(ParserI) #","title":"parser\u7684\u5b9e\u73b0\u5206\u6790"},{"location":"NLTK/implementation/parse/#parser","text":"tree.py probability.py grammar.py parse","title":"parser\u7684\u5b9e\u73b0\u5206\u6790"},{"location":"NLTK/implementation/parse/#_1","text":"\u6839\u636e\u7528\u6237\u6307\u5b9a\u7684grammar\uff0c\u6784\u9020parser\u3002","title":"\u529f\u80fd\u5206\u6790"},{"location":"NLTK/implementation/parse/#_2","text":"CFG PCFG( probabilistic context free grammar ) \u9996\u5148\u9700\u8981\u8003\u8651\u7684\u5982\u4f55\u6765\u8868\u793a CFG ? \u7531 grammar.py \u5b9e\u73b0\u3002","title":"\u652f\u6301\u7684\u8bed\u6cd5"},{"location":"NLTK/implementation/parse/#nonterminal","text":"class Nonterminal A non-terminal symbol for a context free grammar, immutable and hashable class FeatStructNonterminal","title":"Nonterminal"},{"location":"NLTK/implementation/parse/#productions","text":"class Production class DependencyProduction(Production) A dependency grammar production class ProbabilisticProduction","title":"Productions"},{"location":"NLTK/implementation/parse/#grammars","text":"class CFG A context-free grammar. A grammar consists of a start state and a set of productions. class FeatureGrammar(CFG) A dependency grammar production class DependencyGrammar class ProbabilisticDependencyGrammar","title":"Grammars"},{"location":"NLTK/implementation/parse/#grammar","text":"","title":"grammar"},{"location":"NLTK/implementation/parse/#class-recursivedescentparserparseri","text":"","title":"class RecursiveDescentParser(ParserI)"},{"location":"NLTK/implementation/parse/#class-transitionparserparseri","text":"","title":"class TransitionParser(ParserI)"},{"location":"NLTK/implementation/parse/#class-bottomupprobabilisticchartparserparseri","text":"","title":"class BottomUpProbabilisticChartParser(ParserI)"},{"location":"NLTK/implementation/parse/#class-maltparserparseri","text":"","title":"class MaltParser(ParserI)"},{"location":"NLTK/implementation/parse/#class-genericstanfordparserparseri","text":"","title":"class GenericStanfordParser(ParserI)"},{"location":"NLTK/implementation/parse/#class-viterbiparserparseri","text":"","title":"class ViterbiParser(ParserI)"},{"location":"NLTK/implementation/parse/#class-chartparserparseri","text":"","title":"class ChartParser(ParserI)"},{"location":"NLTK/implementation/parse/#class-genericcorenlpparserparseri-tokenizeri-taggeri","text":"","title":"class GenericCoreNLPParser(ParserI, TokenizerI, TaggerI)"},{"location":"NLTK/implementation/parse/#class-bllipparserparseri","text":"","title":"class BllipParser(ParserI)"},{"location":"NLTK/implementation/parse/#class-shiftreduceparserparseri","text":"","title":"class ShiftReduceParser(ParserI)"},{"location":"NLTK/implementation/parse/nltk.parser/","text":"Grammar Parsing # http://www.nltk.org/howto/grammar.html https://github.com/nltk/nltk/tree/develop/nltk/parse http://www.nltk.org/book/ch08.html","title":"[Grammar Parsing](http://www.nltk.org/howto/grammar.html)"},{"location":"NLTK/implementation/parse/nltk.parser/#grammar-parsing","text":"http://www.nltk.org/howto/grammar.html https://github.com/nltk/nltk/tree/develop/nltk/parse http://www.nltk.org/book/ch08.html","title":"Grammar Parsing"},{"location":"NLTK/implementation/parse/nltk.tree/","text":"nltk.tree.Tree # \u76f8\u5173\u6587\u6863\u6709\uff1a Unit tests for nltk.tree.Tree \u770b\u4e86\u5b83\u7684\u5b9e\u73b0\u540e\uff0c\u53d1\u73b0\u5b83\u7684\u5b9e\u73b0\u65b9\u5f0f\u548c\u666e\u901a\u7684\u5b9e\u73b0\u6811\u7684\u65b9\u5f0f\u4e0d\u540c\uff1a \u666e\u901a\u7684\u5b9e\u73b0\u65b9\u5f0f\u90fd\u662f\u5b9a\u4e49 class Node \uff0c\u7136\u540e Node \u4e2d\u5305\u542b\u6307\u5411\u5b50\u8282\u70b9\u7684\u6307\u9488\uff0c\u8fd9\u79cd\u65b9\u5f0f\u662f\u5c06\u6811\u7684\u6784\u6210\u770b\u505a\u662f Node \u4e4b\u95f4\u7684\u5173\u8054\uff1b \u800c\u5b83\u663e\u793a\u5730\u5b9a\u4e49 class Node \uff0c\u76f4\u63a5\u5b9a\u4e49 class Tree \uff0c\u5b83\u5c06\u6811\u7684\u6784\u6210\u770b\u505a\u662f Tree \u4e4b\u95f4\u7684\u5173\u8054\u3002 class Tree(list): def __init__(self, node, children=None): if children is None: raise TypeError( \"%s: Expected a node value and child list \" % type(self).__name__ ) elif isinstance(children, string_types): raise TypeError( \"%s() argument 2 should be a list, not a \" \"string\" % type(self).__name__ ) else: list.__init__(self, children) self._label = node \u663e\u7136\uff0c\u867d\u7136 class Tree \u53eb\u505a Tree \uff0c\u4f46\u662f\u5b9e\u9645\u4e0a\uff0c\u5b83\u7684\u529f\u80fd\u975e\u5e38\u7c7b\u4f3c\u4e8e Node \u3002 \u53ef\u4ee5\u770b\u5230\uff0c class Tree \u7ee7\u627f\u4e86 list \uff0c\u5b83\u5c06\u548c\u5b50\u6811\u4e4b\u95f4\u7684\u5173\u8054\u5168\u90e8\u4fdd\u5b58\u4e0e list \u4e2d\u3002 \u53e6\u5916\u5b83\u53ea\u6709\u4e00\u4e2a\u6210\u5458\u53d8\u91cf _label \uff0c\u8fd9\u662f\u56e0\u4e3a class Tree \u4e3b\u8981\u7528\u4f5cparse tree\uff0cparse tree\u4e2d\u7684\u6bcf\u4e2anode\u53ea\u6709\u4e00\u4e2alabel\u3002 \u4e0b\u9762\u662f\u4ee3\u7801\u4e2d\u7ed9\u51fa\u7684\u8be5\u7c7b\u7684\u6587\u6863\uff1a A Tree represents a hierarchical grouping of leaves and subtrees. For example, each constituent in a syntax tree is represented by a single Tree. A tree's children are encoded as a list of leaves and subtrees, where a leaf is a basic (non-tree) value; and a subtree is a nested Tree. \u4ece\u4e0a\u8ff0\u6587\u6863\u4e2d\uff0c\u53ef\u4ee5\u770b\u51fa class Tree \u548c\u666e\u901a\u7684\u6811\u7684\u5b9e\u73b0\u65b9\u5f0f\u4e4b\u95f4\u7684\u4e00\u4e2a\u5dee\u5f02\uff1a \u5728\u666e\u901a\u7684\u5b9e\u73b0\u4e2d\uff0c\u6811\u4e2d\u8282\u70b9\u7684\u7c7b\u578b\u90fd\u662f\u76f8\u540c\u7684\uff0c\u8fd9\u91cc\u628a\u5b83\u79f0\u4e3a class Node \uff0c\u65e0\u8bba\u662f\u5185\u8282\u70b9\u8fd8\u662f\u53f6\u5b50\u8282\u70b9\uff1b \u4f46\u662f\u5728 class Tree \u4e2d\uff0c\u6ca1\u6709\u6309\u7167\u8fd9\u79cd\u65b9\u5f0f\u5b9e\u73b0\uff0c\u800c\u662f\u6240\u6709\u7684\u5185\u8282\u70b9\u7684\u5185\u7701\u90fd\u662f class Tree \uff0c\u800c\u53f6\u5b50\u8282\u70b9\u7684\u7c7b\u578b\u662fbasic (non-tree) type\u3002","title":"[nltk.tree.Tree](https://github.com/nltk/nltk/blob/develop/nltk/tree.py)"},{"location":"NLTK/implementation/parse/nltk.tree/#nltktreetree","text":"\u76f8\u5173\u6587\u6863\u6709\uff1a Unit tests for nltk.tree.Tree \u770b\u4e86\u5b83\u7684\u5b9e\u73b0\u540e\uff0c\u53d1\u73b0\u5b83\u7684\u5b9e\u73b0\u65b9\u5f0f\u548c\u666e\u901a\u7684\u5b9e\u73b0\u6811\u7684\u65b9\u5f0f\u4e0d\u540c\uff1a \u666e\u901a\u7684\u5b9e\u73b0\u65b9\u5f0f\u90fd\u662f\u5b9a\u4e49 class Node \uff0c\u7136\u540e Node \u4e2d\u5305\u542b\u6307\u5411\u5b50\u8282\u70b9\u7684\u6307\u9488\uff0c\u8fd9\u79cd\u65b9\u5f0f\u662f\u5c06\u6811\u7684\u6784\u6210\u770b\u505a\u662f Node \u4e4b\u95f4\u7684\u5173\u8054\uff1b \u800c\u5b83\u663e\u793a\u5730\u5b9a\u4e49 class Node \uff0c\u76f4\u63a5\u5b9a\u4e49 class Tree \uff0c\u5b83\u5c06\u6811\u7684\u6784\u6210\u770b\u505a\u662f Tree \u4e4b\u95f4\u7684\u5173\u8054\u3002 class Tree(list): def __init__(self, node, children=None): if children is None: raise TypeError( \"%s: Expected a node value and child list \" % type(self).__name__ ) elif isinstance(children, string_types): raise TypeError( \"%s() argument 2 should be a list, not a \" \"string\" % type(self).__name__ ) else: list.__init__(self, children) self._label = node \u663e\u7136\uff0c\u867d\u7136 class Tree \u53eb\u505a Tree \uff0c\u4f46\u662f\u5b9e\u9645\u4e0a\uff0c\u5b83\u7684\u529f\u80fd\u975e\u5e38\u7c7b\u4f3c\u4e8e Node \u3002 \u53ef\u4ee5\u770b\u5230\uff0c class Tree \u7ee7\u627f\u4e86 list \uff0c\u5b83\u5c06\u548c\u5b50\u6811\u4e4b\u95f4\u7684\u5173\u8054\u5168\u90e8\u4fdd\u5b58\u4e0e list \u4e2d\u3002 \u53e6\u5916\u5b83\u53ea\u6709\u4e00\u4e2a\u6210\u5458\u53d8\u91cf _label \uff0c\u8fd9\u662f\u56e0\u4e3a class Tree \u4e3b\u8981\u7528\u4f5cparse tree\uff0cparse tree\u4e2d\u7684\u6bcf\u4e2anode\u53ea\u6709\u4e00\u4e2alabel\u3002 \u4e0b\u9762\u662f\u4ee3\u7801\u4e2d\u7ed9\u51fa\u7684\u8be5\u7c7b\u7684\u6587\u6863\uff1a A Tree represents a hierarchical grouping of leaves and subtrees. For example, each constituent in a syntax tree is represented by a single Tree. A tree's children are encoded as a list of leaves and subtrees, where a leaf is a basic (non-tree) value; and a subtree is a nested Tree. \u4ece\u4e0a\u8ff0\u6587\u6863\u4e2d\uff0c\u53ef\u4ee5\u770b\u51fa class Tree \u548c\u666e\u901a\u7684\u6811\u7684\u5b9e\u73b0\u65b9\u5f0f\u4e4b\u95f4\u7684\u4e00\u4e2a\u5dee\u5f02\uff1a \u5728\u666e\u901a\u7684\u5b9e\u73b0\u4e2d\uff0c\u6811\u4e2d\u8282\u70b9\u7684\u7c7b\u578b\u90fd\u662f\u76f8\u540c\u7684\uff0c\u8fd9\u91cc\u628a\u5b83\u79f0\u4e3a class Node \uff0c\u65e0\u8bba\u662f\u5185\u8282\u70b9\u8fd8\u662f\u53f6\u5b50\u8282\u70b9\uff1b \u4f46\u662f\u5728 class Tree \u4e2d\uff0c\u6ca1\u6709\u6309\u7167\u8fd9\u79cd\u65b9\u5f0f\u5b9e\u73b0\uff0c\u800c\u662f\u6240\u6709\u7684\u5185\u8282\u70b9\u7684\u5185\u7701\u90fd\u662f class Tree \uff0c\u800c\u53f6\u5b50\u8282\u70b9\u7684\u7c7b\u578b\u662fbasic (non-tree) type\u3002","title":"nltk.tree.Tree"},{"location":"spaCy/","text":"\u5173\u4e8e\u672c\u7ae0 # spaCy Speech and Language Processing","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"spaCy/#_1","text":"spaCy Speech and Language Processing","title":"\u5173\u4e8e\u672c\u7ae0"}]}